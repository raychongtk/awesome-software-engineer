{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Awesome Software Engineer <p>Awesome Software Engineer is a community to share software engineering topics and inspires software engineers. Check out my newsletter to receive latest content: https://blog.awesomesoftwareengineer.com/</p>"},{"location":"#support","title":"\u2b50 Support","text":"<p>If you like my content, please</p> <ol> <li>give this repository a star</li> <li>subscribe to my newsletter at Substack</li> <li>share my content to your friends</li> </ol> <p>Buy me a coffee </p>"},{"location":"microservices/","title":"Microservices","text":"<ul> <li>Microservices Documentation</li> <li>The Path to Microservices: Opening Chapter</li> <li>The Path to Microservices: What are Microservices?</li> <li>The Path to Microservices: Do You Really Need Microservices Architecture?</li> <li>The Path to Microservices: CAP Theorem</li> <li>The Path to Microservices: Service Discovery</li> <li>The Path to Microservices: CI/CD</li> <li>The Path to Microservices: API Gateway Aggregation Pattern</li> <li>Microservices ecosystem</li> <li>The pains of microservices - Part 1</li> <li>The pains of microservices - Part 2</li> <li>Distributed tracing and distributed logging</li> </ul>"},{"location":"security/","title":"Security","text":"<ul> <li>What is JWT?</li> <li>OAuth Explained</li> <li>Never Trust Your Frontend When Developing API</li> <li>Designing Authentication for Your Public API Platform</li> <li>How To Protect Your Code On The Client Side?</li> <li>Security Practices: Blocklist vs Allowlist</li> <li>Software Supply Chain Security</li> <li>Passwordless Authentication</li> <li>Secure web application with CSP</li> <li>What is two factors authentication?</li> <li>How does an authenticator work?</li> <li>Hashing vs Encryption vs Encoding</li> <li>SMS Pumping</li> </ul>"},{"location":"api-design/designing-api-with-the-right-granularity/","title":"Designing API with the Right Granularity","text":"<p>I discussed UI Driven API Design in my previous post. It is about the data format. This time, I am going to talk about API Granularity. When deciding on API Granularity, we still need to stand in frontend\u2019s point of view.</p> <p>Let\u2019s take a look at the below wireframe.</p> <p></p> <p>This is a page to edit a user\u2019s profile, for example, there are 4 fields to edit on this page including name, username, email, and address. When the user enters this page, frontend needs to invoke the backend\u2019s API to retrieve the data for name, username, email, and address. After clicking the save button, it will call the Edit Profile API to update the data. So, for this page, actually we need two APIs for frontend.</p> <p></p> <p>Now, we have another page called My Profile. It displays basic user information like username and email only. Other information is something related to the user like order history, payment history, etc.</p> <p>It looks like we can reuse the previous API that is in use on the Edit Profile page to retrieve the user profile. Would you do this? Let\u2019s take a bit of time to think about it.</p> <p>Some backend engineers create coarse-grained API to fit the frontend use cases as more as possible. That said, they don\u2019t need to maintain different endpoints in the backend instead they just maintain a coarse-grained endpoint and let the frontend developer choose the data they need.</p> <p>Another type of engineer, they create fine-grained API to fit the particular use case in the frontend which is what I mentioned before, UI Driven API Design. This type of engineer will create an endpoint according to the UI design. If we use the above example, this type of engineer will definitely create another API that only returns username and email to the frontend.</p> <p>Is it good to return all the data to the frontend and let the frontend reuse that API and choose the data that they need? Personally, my answer is no.</p> <p>Let\u2019s stand from a frontend developer\u2019s angle to look at compare the coarse-grained and fine-grained API.</p> <p>If we use coarse-grained API, let\u2019s say we return 10 fields but the frontend only needs 2 fields. The frontend developer might confuse why does the backend developer return 10 fields to me? What fields should I use? It might increase some extra communication costs between the frontend and the backend. But if we use fine-grained API and only return 2 fields, the frontend developer will know which field they need to use because there is no extra noise in the API response.</p> <p>Fine-grained API is not a silver bullet, for example, we need to manage a lot of similar endpoints if we have a lot of similar use cases in the frontend. But if your system scale is that large, then you might consider other technologies to solve this complexity instead of using RESTful API.</p> <p> <p>Buy me a coffee </p>"},{"location":"api-design/do-you-use-pagination-right%3F/","title":"Do You Use Pagination Right?","text":"<p>Pagination is a valuable approach for enhancing data retrieval performance. In a previous post, I discussed the implementation of Pagination in API design. However, it\u2019s important to be aware of a potential pitfall associated with this pagination technique. In software engineering, there are two primary types of Pagination: Offset-based Pagination and Cursor-based Pagination.</p>"},{"location":"api-design/do-you-use-pagination-right%3F/#offset-based-pagination","title":"Offset-based Pagination","text":"<p>Offset-based pagination, the technique I previously introduced, offers simplicity in design and flexibility for navigating to any desired page.</p> <p></p> <p>This visualizes how offset-based pagination navigates pages. With this design, users can simply jump to any page that they want.</p> <p></p> <pre><code>select * from target_table where x &gt; y order by id skip 0 limit 8;\n\nselect * from target_table where x &gt; y order by id skip 8 limit 8;\n\nselect * from target_table where x &gt; y order by id skip 16 limit 8;\n\nselect * from target_table where x &gt; y order by id skip 24 limit 8;\n\nselect * from target_table where x &gt; y order by id skip 32 limit 8;\n</code></pre> <p>However, as the offset value increases, retrieving subsequent pages becomes slower. This is because each retrieval requires skipping a certain number of records, resulting in performance degradation. This impact can be especially obvious when working with large datasets or when the offset values are high. It\u2019s crucial to consider these limitations and explore alternative pagination techniques to mitigate the performance issues associated with offset-based pagination.</p> <p>If your system uses offset-based pagination and suffer performance issues, you should talk to your product manager to set a limitation on how deep a user can navigate between pages. The more pages that a user skips, the slower the response is.</p> <p>Or, consider using the below alternative to design pagination.</p>"},{"location":"api-design/do-you-use-pagination-right%3F/#cursor-based-pagination","title":"Cursor-based Pagination","text":"<p>To address these challenges, an alternative pagination approach called cursor-based pagination emerges as a viable solution. Unlike offset-based pagination, cursor-based pagination offers improved performance and stability by leveraging unique cursor values associated with each record. It enables more efficient and consistent queries, especially when dealing with large datasets.</p> <p></p> <p>In cursor-based pagination, random page navigation is not allowed. Instead, only linear access is allowed in this design which users can only navigate to the previous page or the next page. It can provide better performance compared to offset-based pagination, especially when dealing with large datasets.</p> <p></p> <pre><code>select * from target_table where x &gt; y order by id limit 8;\n\nselect * from target_table where x &gt; y and id &gt; 8 order by id limit 8;\n\nselect * from target_table where x &gt; y and id &gt; 16 order by id limit 8;\n\nselect * from target_table where x &gt; y and id &gt; 24 order by id limit 8;\n\nselect * from target_table where x &gt; y and id &gt; 32 order by id limit 8;\n</code></pre> <p>However, it\u2019s important to note that implementing cursor-based pagination can introduce additional complexity compared to offset-based pagination. Maintaining and managing unique cursor values for each record requires careful consideration to ensure their integrity and uniqueness throughout the pagination process.</p> <p>jOOQ did a comparison for the performance. If you are interested in the detailed comparison, go check out this blog.Ref: https://blog.jooq.org/faster-sql-paging-with-jooq-using-the-seek-method/</p>"},{"location":"api-design/do-you-use-pagination-right%3F/#conclusion","title":"Conclusion","text":"<p>When deciding which pagination technique to employ, it\u2019s crucial to evaluate the specific requirements and characteristics of your application. Consider factors such as dataset size, user navigation patterns, and the trade-offs between performance and implementation complexity. By carefully analyzing these factors, you can determine the most suitable pagination approach for optimizing data retrieval performance in your software system.</p> <p> <p>Buy me a coffee </p>"},{"location":"api-design/everything-about-restful-api/","title":"Everything About RESTful API","text":"<p>Today, I\u2019ll be covering the topic of RESTful API design. In this post, I will focus on RESTful API design. I will not compare RPC and RESTful that many. Okay, let\u2019s get started!</p> <p>RESTful is an API development style that focuses on resources. Compared to the RPC style, there is a difference between these two approaches. RESTful is a resource-based style while RPC is an action-based style. For example, if you want to delete a user, your API might look like this:</p> <p>RESTful: /api/user/1 with HTTP method DELETE</p> <p>RPC: ?action=deleteUser&amp;id=1 with HTTP method POST/GET</p> <p>Personally, I think RESTful is more clean and simple compared to RPC. The core concept of RESTful is that using the HTTP methods to replace the action verb and the URL path can focus on the resource itself. If the APIs are related to CRUD (create, read, update, delete), then RESTful will be your best friend. Because RESTful is a CRUD-friendly and resource-oriented development style.</p>"},{"location":"api-design/everything-about-restful-api/#stateless","title":"Stateless","text":"<p>RESTful API is stateless. That said, the server does not need to store any state. The client will provide all the information for the server to understand it. This design helps the application to scale easily because each request response is independent. It means there is no dependency on a specific backend application and we can scale the backend applications to support more traffic easily.</p>"},{"location":"api-design/everything-about-restful-api/#resource","title":"Resource","text":"<p>I mentioned the word \u201cResource\u201d many times, so what is a resource?</p> <p>Basically, a resource means a business domain in your project and it must be a noun. For example, the user can be a resource, and the order can be another resource. It depends on how you split the business into different domains. After that, you need CRUD operations to support these domains. In RESTful API style, CRUD will map to the following HTTP methods: GET, POST, PUT, PATCH, and DELETE.</p>"},{"location":"api-design/everything-about-restful-api/#get","title":"GET","text":"<p>GET is used for making a Read request to the server and it is idempotent. For example, /api/user/1 with GET HTTP method will return the id 1 user back to the client. Instead of using path param, we can also use query param for the advanced query.</p>"},{"location":"api-design/everything-about-restful-api/#post","title":"POST","text":"<p>POST is used for making a Create request to the server. Every time you make a POST request, it will create a resource in the server. For example, /api/user with POST HTTP method and request body will create a new user.</p>"},{"location":"api-design/everything-about-restful-api/#put","title":"PUT","text":"<p>PUT is used for making an Update request to the server and it is idempotent. Concretely, PUT is referring to replacing a resource. That said, the incoming request will replace the whole existing resource. For example, /api/user/1 with PUT HTTP method and request body will replace the id 1 user in the database. If you have 5 fields in the user domain, then they will be replaced by the incoming request.</p>"},{"location":"api-design/everything-about-restful-api/#patch","title":"PATCH","text":"<p>PATCH is used for making an Update request to the server but it is different from PUT and it is not designed to be idempotent. A patch request, it is used to update particular fields in a domain. For example, updating the email address from a user domain. Only email will be sent in the request body. The URL might look like this: /api/user/1 with PATCH HTTP method.</p>"},{"location":"api-design/everything-about-restful-api/#delete","title":"DELETE","text":"<p>DELETE is used for making a Delete request to the server and it is idempotent. Sending a request like this /api/user/1 with HTTP method DELETE will delete the id 1 user in the database.</p>"},{"location":"api-design/everything-about-restful-api/#http-response-status-code","title":"HTTP Response Status Code","text":"<p>RESTful uses HTTP status codes to identify the API status. For example, 200 (OK), 201 (Created), 204 (No Content), 400 (Bad Request), 500 (Internal Server Error), etc.</p> <p>To understand more about HTTP Response Status Code, you can refer to Mozilla.</p> <p>Personally, I think don\u2019t struggle with semantics. After you learn the semantics of the HTTP methods and the RESTful convention, you should think about how to utilize it for better API design instead of struggling with literal meaning. Never take a theory as dogma because there are always exceptional cases. For example, what HTTP method will you use for the login API? How about search API?</p> <p>These scenarios are not just simple CRUD operations, you can\u2019t use the above theory to adopt the HTTP method. </p> <p>For the login scenario, POST is acceptable because we need to put the login credentials into the request body for security reasons.</p> <p>How about the search scenario? I\u2019ll leave it for you.</p>"},{"location":"api-design/everything-about-restful-api/#query-param-vs-path-param","title":"Query Param vs Path Param","text":"<p>I\u2019ve mentioned the query param and path param above, here is a visualization for you in case you don\u2019t know what is query param and path param.</p> <p></p> <p>In general, a unique identifier will be placed in the path param to identify a resource. For query param, it is for conditional searching in most of the scenarios.</p>"},{"location":"api-design/everything-about-restful-api/#idempotency","title":"Idempotency","text":"<p>I\u2019ve mentioned \u201cidempotent\u201d many times. You may have a doubt about this word. I have the same feeling back in 2019 when I was a fresh graduate. Actually, idempotency is used to describe whether an API is capable for retry or not after an API request failed. If an API is idempotent (GET, PUT, or DELETE), it means you can retry the API anytime. That said, no matter how many times you send identical requests to the server, it will always return the same result (no side effect).</p>"},{"location":"api-design/everything-about-restful-api/#hateoas-hypermedia-as-the-engine-of-application-state","title":"HATEOAS (Hypermedia as the Engine of Application State)","text":"<p>There is a technique called HATEOAS-driven RESTful API. Simply put, it is a mechanism to allow the client to dynamically navigate to the resources based on the response header. That said, the API response will tell the client what is the next step after calling the API.</p>"},{"location":"api-design/everything-about-restful-api/#conclusion","title":"Conclusion","text":"<p>To sum up, I think the core spirit of RESTful API design is how you utilize the URI to describe the resource relationship and level and make use of the HTTP convention to further describe the action of the API. Also, there is no best API development approach, it will be the best only when it is the best option for the scenario.</p> <p> <p>Buy me a coffee </p>"},{"location":"api-design/how-to-handle-api-backward-compatibility%3F/","title":"How to Handle API Backward Compatibility?","text":"<p>Handling backward compatibility is super important in modern software development. Without backward compatibility, a system cannot support zero downtime deployment.</p> <p>To handle backward compatibility, I can simply categorize the solutions into these 3 types in general.</p>"},{"location":"api-design/how-to-handle-api-backward-compatibility%3F/#create-a-new-flow","title":"Create a new flow","text":"<p>Creating a new flow and making multiple versions available for an API is one of the techniques to handle backward compatibility.</p> <p>With a new API version, the backend is able to support multiple versions at the same time. If the users are using an old client, the requests will be sent to the previous version. If they already updated the new client, then the requests will send to the new version.</p> <p>This approach is suitable for the use case that the whole API data structure or behavior is completely changed. For example, we have an API to send SMS messages to different receivers. In our first API iteration, we use a flattened data structure to represent multiple messages. After a few iterations, we decided to make the API contract more intuitive to reduce cognitive load. So, we want to iterate our API design and change the API contract.</p> <p>What will happen if we change the existing API contract directly? The old client will be gone. Because we no longer support the old API contract. This will affect all the old clients and it is not acceptable in modern software development.</p> <p>So, how can we do better? Create a new flow and deprecate the old API contract but still support it for a period.</p> <p></p> <p>In the above design, we need to support these 2 versions running in production at the same time until your team decided to remove Version 1 completely. With this design, the old client will send requests to Version 1 and the new client will send requests to Version 2. That said, we don\u2019t need to affect the traffic that is sent from the old client and created a new flow to support the new client.</p>"},{"location":"api-design/how-to-handle-api-backward-compatibility%3F/#make-a-new-data-field-optional","title":"Make a new data field optional","text":"<p>Another use case is that the data structure remains the same but we want to add a new field to the API contract. In this scenario, we can just simply add a new field to the API contract and keep it optional. By doing this, we can check if the request has assigned a value to this new field or not to determine whether we need to run a special logic to support this new behavior. The requests from the old client do not need to run the special logic related to the new field.</p>"},{"location":"api-design/how-to-handle-api-backward-compatibility%3F/#give-a-new-data-field-a-default-value","title":"Give a new data field a default value","text":"<p>This approach is quite similar to the second option. The only difference is that we already have the value for the new field even if API consumers didn\u2019t assign any. For example, we want to add a new field called subscriptionType. Because you want to change your business model from completely free to support both free and premium users. So, this field will be an enum value containing FREE and PREMIUM. In this scenario, we already know the existing users are in FREE subscription model, so we can simply assign FREE as the default value for the subscriptionType. If the request payload does not assign any value to subscriptionType, by default it will be FREE. With this approach, even if we introduce a new data field, it will not affect the existing users. Because the API behavior is updated seamlessly inside the backend without bothering users.</p>"},{"location":"api-design/how-to-handle-api-backward-compatibility%3F/#conclusion","title":"Conclusion","text":"<p>Let\u2019s wrap it up. Backward compatibility is important for modern software development and every software engineer has to care about making API backward compatible. Without backward compatibility, your users will complain about your service because every time you make a new change in the API contract will affect the existing users directly and they know you are doing something wrong. It will hurt your professional image and the reliability of your service.</p> <p> <p>Buy me a coffee </p>"},{"location":"api-design/improve-performance-when-retrieving-large-dataset/","title":"Improve Performance When Retrieving Large Dataset","text":"<p>In the realm of software development, backend systems store vast amounts of data, and it is the responsibility of software engineers to design APIs that retrieve this data from databases.</p> <p>When dealing with small datasets, some novice engineers may design APIs that fetch all the data from the database in a single operation. This approach works fine with small data sizes since it doesn\u2019t lead to performance issues.</p> <p></p> <p>However, when the database contains millions of records, retrieving all the data in one go can result in slow query performance or even OOM. Slow API responses not only hinder user experience but also undermine the reliability of the API, potentially eroding customer trust.</p> <p></p> <p>To address this issue and improve performance, a recommended solution is to redesign the API using pagination.</p> <p></p> <p>In this enhanced design, a large dataset is divided into smaller chunks, and the data is retrieved incrementally using pagination. For instance, the first API call fetches the first 8 records from the initial page. If users need to access the second page, another API call is made to retrieve that specific page.</p> <p></p> <p>By adopting this approach, the API retrieval performance can be optimized. In contrast to the previous method, which retrieved the entire dataset at once, the improved design fetches only 8 records per API call, effectively controlling the data size and enhancing performance.</p> <p>\ud83d\udca1 Do you know what\u2019s the hidden performance issue with using this type of pagination? Share your thoughts in the comment section!</p> <p>In summary, pagination is a technique that divides large datasets into manageable pages, improving retrieval performance, controlling data size, and enhancing the user experience when dealing with substantial amounts of data.</p> <p> <p>Buy me a coffee </p>"},{"location":"api-design/let-ui-drive-api-design/","title":"Let UI Drive API Design","text":"<p>API development is a day-to-day routine for every backend developer. When developing API for frontend developers, what approach will you use? </p> <p>This is really important when backend developers collaborate with frontend developers. If the API contract is convenient, then it will facilitate frontend developers\u2019 work and provide a better developer experience.</p> <p>To explain more about the API development approach, let\u2019s take the below UI as an example:</p> <p></p> <p>Imagine we have a page with a basic table in the frontend. In this table, each row is a template and each template has multiple translations for different languages. That said, each column is a translation for that template. When a user clicks the \u201cclick to view\u201d button, it will display the translation content.</p> <p>So, how do backend developers store the template data in the database? Let\u2019s keep everything simple. We don\u2019t consider normalization and other functionalities. We just focus on the template.</p> <p></p> <p>Based on this table design, each row will be a template translation. If we search the template by template_id, we can get all the translations for this template.</p> <p>Now, the problem is how would you design a good API contract for the frontend developers?</p> <p></p> <p>Would you design the API like this? This design is using data to drive the API design. Each JSON object is a row in the database and eventually, we return a list of templates to the frontend.</p> <p>Do you think it is a good design? Yes or no. If you are frontend developer and the backend guy gives you this API contract and you look at the UI design, I think the first thing that comes to your mind is to aggregate the data. Because this design does not have any data aggregation, it is just purely retrieving data from the database and returning it to the frontend.</p> <p>From the frontend developers\u2019 perspective, they don\u2019t care how the backend stores the data as long as it is reliable and correct. When the frontend receives the data, the frontend developer needs to do the aggregation in order to fit the UI design. It seems a bit weird. Should frontend do this aggregation? My answer is no.</p> <p>Using this design, the developer experience for the frontend developers is bad and the API itself is not intuitive from a UI perspective.</p> <p>Personally, when I develop APIs, I will start from a UI perspective in terms of providing a better developer experience for frontend developers and making them happy. Because they are the API consumer, as a backend developer, I need to make them feel convenient when using my APIs. I don\u2019t know whether there is terminology for this approach or not but I will call it UI Driven API Design.</p> <p></p> <p>If we look at the diagram above, we can know there is User Experience and Developer Experience between different layers. From user to UI, that is User Experience. On the other hand, there is a Developer Experience between UI and API. The user experience drives the UI and the developer experience drives the API. That said, If the backend developers can help to aggregate the data and provide a better API shape to the frontend developers, we can optimize the developer experience for the frontend developers.</p> <p>So we need to let the UI drive the API design. If we look at the first diagram, we can know the data is aggregated by the template. Each row is a template followed by different translations. With this UI presentation, we can reshape the API contract to the below format.</p> <p></p> <p>Using this approach, the backend API already helped the frontend developer aggregates the data by template and the frontend developer can use the data directly without aggregating the data in the frontend. Now, each template will get a list of translations in the API contract. It seems to be more intuitive from a UI perspective.</p> <p>With this API contract, each field is a template and contains a list of translations. When the user clicks the translation button to view the translation, the frontend developer can get the translation content from the list by index. Also, the frontend developer can get the correct row by looking up the template id (key) in the map data structure to get the list of translations (value).</p> <p>Let\u2019s wrap it up. When designing API, try to understand the developer usage habits or UI and talk to your consumers to understand their needs. Always start from the API consumers\u2019 side, it will help you design a better API contract.</p> <p>Do you have any other ideas to improve the developer experience between Frontend and Backend? Share your ideas in the comment section!</p> <p> <p>Buy me a coffee </p>"},{"location":"api-design/optimize-api-through-parallelization/","title":"Optimize API Performance Through Parallelization","text":"<p>Parallelization is useful in such a scenario where a single request from the frontend needs N requests in the backend to compose the data that the frontend needs.</p> <p></p> <p>Imagine when internal users trigger a \u201ccreate order\u201d request in an internal portal, and the \u201ccreate order\u201d request involves 10 requests from different microservices to complete the action. If one request takes 1 second to complete, it takes 10 seconds in this case.</p> <p>Triggering a request in the frontend and taking 10 seconds to respond back to the frontend is really hurting UX a lot.</p> <p>Let\u2019s take a step back. Do we really need to execute the requests sequentially? The requests do not have a dependency, they can all be executed in parallel. In this case, if we change the design to parallel requests, the performance will be improved significantly.</p> <p></p> <p>With parallel requests design, every request is assigned to a dedicated thread and executed concurrently. After submitting to a thread pool, the program waits for all the results to come back. When all the requests are completed, the program aggregates the response and returns it back to the frontend. In this case, 10 seconds waiting time can be reduced to 1 second in the best case as we don\u2019t need to execute the request one by one. Instead, we execute the 10 requests to 10 different services simultaneously.</p> <p>What if we have a dependency between some requests? In this case, we can mix serialization and parallelization together.</p> <p></p> <p>When mixing serialization and parallelization together, we can still optimize the processing time a lot. But this is not going to work when the requests are tightly coupled with each other. Because you always need to wait for a response in order to proceed with the logic flow. In this case, you may consider other optimization strategies.</p> <p>In summary, if you have an API that needs to integrate with many services and they don\u2019t have a dependency on each other. You can consider this optimization approach in order to reduce the waiting time.</p> <p> <p>Buy me a coffee </p>"},{"location":"api-design/optimize-api-via-batch-requests/","title":"Enhancing API Performance Through Batch Requests","text":"<p>When designing an API, tailoring it to the specific use case is of utmost importance. This involves considering two primary design approaches: Single Requests and Batch Requests. Each approach has its own set of advantages and disadvantages, and making the right choice depends on the scenarios you encounter.</p> <p></p>"},{"location":"api-design/optimize-api-via-batch-requests/#single-requests","title":"Single Requests","text":"<p>In the Single Requests approach, API consumers initiate multiple requests independently. For instance, if you need to send inbox notifications to various users, under this design, each request must be sent one after the other. However, this method may not be the most efficient. Each request incurs network overhead and necessitates waiting for individual responses. Particularly in scenarios where the same request needs to be repeatedly sent to the API provider, this approach becomes inefficient and resource-intensive.</p>"},{"location":"api-design/optimize-api-via-batch-requests/#batch-requests","title":"Batch Requests","text":"<p>On the other hand, the Batch Requests design streamlines the process. When, for instance, inbox notifications need to be sent to a hundred users, this approach requires only a single invocation of the notification API. This optimizes network overhead significantly. Yet, it introduces complexities, such as managing invalid requests within the batch and the intricacies of the overall design. Error handling and determining the source of failures become more intricate when compared to the Single Requests design.</p>"},{"location":"api-design/optimize-api-via-batch-requests/#the-right-choice","title":"The Right Choice","text":"<p>Choosing between these two approaches isn\u2019t straightforward and depends on the specific context. Neither approach is universally superior; rather, each has its strengths for particular use cases. The decision hinges on factors such as the nature of the operations, the frequency of requests, and the balance between network efficiency and design complexity.</p> <p></p> <p>This is a simplified mental model that could help you choose the right API design. To find the optimal solution, consider a hybrid approach. Employ Single Requests for simple, critical operations, and leverage Batch Requests for tasks involving a large volume of requests that can benefit from reduced network overhead. Ultimately, the best approach aligns with your application\u2019s unique requirements and goals.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/4-popular-cache-patterns/","title":"4 Popular Cache Patterns","text":"<p>In this post, we\u2019ll cover 4 popular cache patterns including Cache-aside, Read-through, Write-through, and Write-back patterns. The application performance will be improved a lot if you choose the right cache strategy. Let\u2019s jump into the topic.</p>"},{"location":"architecture/4-popular-cache-patterns/#cache-aside","title":"Cache-aside","text":"<p>The cache-aside pattern is good for the read-heavy scenario. The control of reading from cache or database is on the application side. Your application controls the flow of reading data. If a cache is hit, read the data from the cache, otherwise, read the data from the database and write the data back to the cache. There is a data inconsistency issue under this design since the data is written to the database directly. After writing the data to the database,  the cache does not know the value in the database gets updated. To solve this problem, we usually delete the cache key after we write to the database or we can use cache write strategies.</p> <p>Example: Redis</p>"},{"location":"architecture/4-popular-cache-patterns/#side-topic","title":"Side Topic","text":"<p>Do you know why we don\u2019t remove the cache key first and write to the database afterward?</p>"},{"location":"architecture/4-popular-cache-patterns/#read-through","title":"Read-through","text":"<p>The read-through pattern is also good for the read-heavy scenario. The idea in this pattern is quite similar to the cache-aside pattern. The only difference is that the flow of reading data is controlled by the cache library instead of your application. That said, your application only needs to read the data from the cache directly and the cache library will help you manage all the stuff related to cache hit or cache miss. In this pattern, we have the same issue that faced in the cache-aside pattern.</p> <p>Example: Apache Ignite, Amazon DynamoDB Accelerator</p>"},{"location":"architecture/4-popular-cache-patterns/#write-through","title":"Write-through","text":"<p>With the write-through pattern, the data is first written into the cache and the cache library will help us sync the data to the database. Your application will not touch the database directly. Instead, every write will go through the cache. This will introduce a bit of latency during the data propagation. To solve this problem, the write-through pattern is always put together with the read-through pattern. It can solve the problem in the read-through or cache-aside pattern. Also, the read-through pattern solves the latency problem in the write-through pattern as well because now the data read and write is always in the cache.</p> <p>Example: Apache Ignite, Amazon DynamoDB Accelerator</p>"},{"location":"architecture/4-popular-cache-patterns/#write-back","title":"Write-back","text":"<p>The write-back pattern is good for the write-heavy scenario. With this pattern, the data will be constantly written into the cache. After a certain time, the cache will be written back to the database once. It can reduce the overall write to the database and mitigate database workload. It facilitates the application performance when you are in a write-heavy use case but it does give some drawbacks like data inconsistency and data loss if the cache is down and the data haven\u2019t been written back to the backing store.</p> <p>To mitigate the problems from the write-back pattern, simply we can combine the read-through pattern to access the latest data in the cache to solve the consistency problem. Also, we can put more replicas to mitigate the situation of potential data loss.</p> <p>Example: Apache Ignite, Amazon DynamoDB Accelerator, Redis</p>"},{"location":"architecture/4-popular-cache-patterns/#case-study","title":"Case study","text":"<p>I was asked to solve a problem with a write-heavy scenario in an interview. It was related to frequent geolocation updates and the use case is writing a lot of user locations into the database frequently, because of frequent updates, the database becomes a bottleneck. I was using the write-back strategy to solve this problem back then because the use case does not need a real-time behavior. That said, latency is accepted in that situation, so the write-back strategy is one of the possible solutions for that.</p>"},{"location":"architecture/4-popular-cache-patterns/#references","title":"References","text":"<ul> <li>https://aws.amazon.com/blogs/database/amazon-dynamodb-accelerator-dax-a-read-throughwrite-through-cache-for-dynamodb/</li> <li>https://ignite.apache.org/docs/latest/persistence/external-storage</li> </ul> <p> <p>Buy me a coffee </p>"},{"location":"architecture/api-vs-webhook/","title":"API vs Webhook","text":"<p>I guess so many engineers from the tech industry were confused by API and Webhook. Today, I will explain this topic and clear your doubt.</p> <p></p> <p>Imagine System A is your application and System B is an external system that your system depends on. Now, System A needs the data from System B to process its business logic, in general, it will be a status of a record. Now, let\u2019s compare API and Webhook.</p>"},{"location":"architecture/api-vs-webhook/#api","title":"API","text":"<p>If System B provides an API for System A to query its data, System A will need to control when to query System B and System B might not have data when System A invokes the API. That said, it is an HTTP request-response pattern.</p> <p>Every time System A invokes System B, there are 2 possible results; containing data or nothing. If there is data in the response, System A will do some processing according to the data to execute its own business logic. If not, System A will ignore it and wait for the next invocation.</p> <p>Do you know what are the problems in this approach? I\u2019ll leave it to you. Think about it.</p>"},{"location":"architecture/api-vs-webhook/#webhook","title":"Webhook","text":"<p>Using Webhook, System A does not rely on the HTTP request-response pattern and System B does not need to provide an API for System A. Instead, System A will provide an API for System B as a Webhook invocation.</p> <p>Yeah\u2026 Actually, Webhook is just an API but the position between System A and System B is reversed. Now, the API provider is not System B but System A. That said, System A does not need to care when it needs to invoke System B. Instead, it just provides an endpoint for System B to send events.</p> <p>Every time something changed on System B, it will send an event to the Webhook URL, and System A will receive it and do its own business handling.</p> <p>The Webhook technique is quite common in some SaaS providers like Stripe, Github, SendGrid, etc. Because when sending data to these services, your system needs to know the status of your records and do some business handling accordingly.</p>"},{"location":"architecture/api-vs-webhook/#security-brainstorm","title":"Security Brainstorm","text":"<p>Now, your system is the API provider, so you need a mature approach to authenticate the incoming requests, else everyone can push notifications to your system via the Webhook.</p> <p>Basically, a common authentication approach is to use a data signature approach. In our use case, System B will provide some rules for System A to verify that the requests are from System B.</p> <p>For details please refer to Stripe Webhook Signature</p>"},{"location":"architecture/api-vs-webhook/#conclusion","title":"Conclusion","text":"<p>Let\u2019s wrap it up. The main difference between API and Webhook is the interaction approach. API is a pull model and Webhook is a push model. Webhook makes the service provider proactively share the information that their client needs but API does not. Choosing API and Webhook wisely can help to improve your system stability and resource utilization.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/architecting-for-simplicity/","title":"Architecting for Simplicity","text":""},{"location":"architecture/architecting-for-simplicity/#architecture","title":"Architecture","text":"<p>Architecture is the foundation of a system and provides rules for consistency, maintainability, and scalability. Features are built on top of the architecture and a flexible architecture allows for easier evolution and extension over time as new features are added.</p> <p>\u201cThere are two ways of constructing a software design: One way is to make it so simple that there are obviously no deficiencies, and the other way is to make it so complicated that there are no obvious deficiencies. The first method is far more difficult.\u201d - Tony Hoare</p> <p>Designing a complicated architecture is a relatively simple task as we can add complexity to solve any problems that arise in the system. However, creating a simple architecture without any visible flaws is quite challenging, as we must strike a balance between functionality and complexity.</p> <p>In order to achieve a simple architecture, it is crucial to have a clear understanding of the problem at hand and establish well-defined boundaries for the architecture. If there are no boundaries, the problem may become exceedingly complex. A simple architecture can reduce the amount of communication required between different engineers, making it easier to maintain, support, and modify. Simplicity also means focusing only on known requirements and avoiding over-prediction of future needs.</p>"},{"location":"architecture/architecting-for-simplicity/#simplicity-is-reliability","title":"Simplicity is Reliability","text":"<p>\u201cSimplicity is a prerequisite for reliability\u201d, said Edsger W. Dijkstra. I agree with that. A complicated architecture can lead to confusion and uncertainty for software engineers, making it difficult to understand what is happening in the system and being afraid of changing the architecture. This can ultimately lead to lower reliability because engineers may not know how to solve issues when they arise. Furthermore, unknown unknowns can demotivate engineers and lead to a cycle of suffering. On the other hand, a simple architecture is easier to understand, maintain, and extend, which can lead to greater reliability. With a clear understanding of how the system works, engineers can quickly identify and resolve issues, improving overall reliability.</p>"},{"location":"architecture/architecting-for-simplicity/#what-leads-to-complicated-architecture","title":"What Leads to Complicated Architecture?","text":""},{"location":"architecture/architecting-for-simplicity/#unclear-requirement","title":"Unclear Requirement","text":"<p>Ambiguous or inaccurate requirements can lead to complicated architecture. Product managers play a crucial role in the Requirements Gathering Process as they are responsible for defining and documenting the system\u2019s requirements. If they don\u2019t have a clear understanding of the system\u2019s responsibilities or they try to imagine user needs, it can lead to a lack of clarity in the requirements.</p> <p>This ambiguity can then lead to a complicated architecture, as developers may have to make assumptions or fill in gaps to try and understand what is required. This can result in a design that is overly complex, hard to maintain, and difficult to extend.</p>"},{"location":"architecture/architecting-for-simplicity/#over-engineering","title":"Over-engineering","text":"<p>Software engineers have a passion for tackling challenges using advanced technologies. When it comes to solving problems in software engineering, there is no one-size-fits-all solution. Often, we have multiple options at our disposal, and we must weigh the pros and cons of different design choices. Some engineers strive for perfection and aim to create an architecture that can handle all scenarios, but this is impractical since every design choice involves trade-offs. Eventually, it leads to a complicated architecture that is hard to maintain and understand.</p>"},{"location":"architecture/architecting-for-simplicity/#unclear-service-responsibility","title":"Unclear Service Responsibility","text":"<p>Ambiguity in service responsibility can lead to complex architecture because software engineers may end up adding different logic to the service without a clear definition of its boundary and responsibility. This can happen because most software engineers prioritize making things work rather than considering maintainability. As a result, the service can become convoluted, with too many functionalities and a lack of clarity on what each part of the service is responsible for. This can cause confusion and difficulties in maintenance and future updates. It\u2019s important to have a clear understanding of service responsibility to ensure a well-designed and maintainable architecture.</p>"},{"location":"architecture/architecting-for-simplicity/#how-to-achieve-architectural-simplicity","title":"How to Achieve Architectural Simplicity?","text":""},{"location":"architecture/architecting-for-simplicity/#demand-clarification","title":"Demand Clarification","text":"<p>Clarifying demands is an important process before designing architecture. It decides how much your team should invest in the whole development process. Therefore, ask questions to clarify business needs! Sometimes, not every line of requirements is needed. So, asking the right questions can definitely help you pull out the demands that are not necessary for the architecture.</p> <p></p> <p>In general, there are 2 types of demands:</p> <ul> <li> <p>Forecast Demand - It is a prediction of what users may want or need in the future. Usually, it expresses \u201cI want\u201d instead of \u201cUser wants\u201d. It is often based on an assumption about what might be useful or desirable.</p> </li> <li> <p>Actual Demand - It is based on real user needs and preferences that are gathered from research, analytics, and feedback. This type of demand reflects what users actually want or need in the system and it should be the primary focus during architecture design.</p> </li> </ul> <p>By focusing on the actual demands, we can ensure the architecture meets the current needs of the system while also leaving some room for future extensions and changes that may be required to meet the forecast demand. It\u2019s important to strike a balance between meeting current needs and preparing for future requirements to avoid creating an architecture that is either too rigid or too complex. This can make sure we don\u2019t invest too much in the prediction.</p>"},{"location":"architecture/architecting-for-simplicity/#complexity-analysis","title":"Complexity Analysis","text":"<p>Once requirements are confirmed, we can design an architecture to support user needs. We might need to deal with 2 types of complexities that are derived from technical.</p> <ul> <li> <p>Accidental Complexity - It is not inherited from the original problem. It is introduced into the system unnecessarily. This complexity can arise due to various reasons such as over-engineering, premature optimization, or lack of experience.</p> </li> <li> <p>Essential Complexity - It is inherited from the original problem. It is the essence of the problem that you need to resolve. This complexity can arise from the problem domain, the business requirements, and the necessary interactions with other systems or components.</p> </li> </ul> <p></p> <p>In general, the goal of architecture design is to manage the essential complexity while minimizing accidental complexity. By doing this, we can simplify the architecture and focus on the original problem instead of dealing with additional technical complexity that is not necessary for the architecture. To achieve this goal, we need to understand the problem well and choose the right technology to solve the right problem. This can ensure we are not wasting our efforts in the wrong direction. Therefore, understanding the trade-off of different design decisions is important.</p>"},{"location":"architecture/architecting-for-simplicity/#separation-of-concerns","title":"Separation of Concerns","text":"<p>Creating good architecture involves considering the separation of concerns, which helps to define clear responsibilities and reduce coupling. Most technical complexity comes from unclear service responsibility and greed. When you want to delight other teams and deliver more value in the service, you will start customization and cause the architecture hard to maintain. For example, when faced with a requirement for a push notification service to allow delayed job submissions for push notifications to be sent after a certain time period, how would you design it?</p> <p>While both putting the delayed job logic in the push notification service or upstream services can achieve the goal, putting the delayed job logic in the push notification service can lead to ambiguity and confusion in the architecture. The responsibility of the push notification service should be solely to send push notifications, not to assist upstream services with task scheduling.</p> <p>The separation of concerns allows for loose coupling, which is a vital principle in good architectural design. Rather than having the scheduling logic tied to the push notification service, each service should have its own scheduling logic built on top of the capability provided by the push notification service. This approach decouples the scheduling logic from the push notification service, making the architecture simpler and easier to maintain.</p>"},{"location":"architecture/architecting-for-simplicity/#visualization","title":"Visualization","text":"<p>Visualization is key in architectural design, it helps to ensure that the knowledge and understanding of the architecture are shared among team members and stakeholders, reducing the risk of confusion and misunderstandings. Therefore, architecture visualization is important to align the ideas between different engineers. With architecture diagrams, the ideas are visualized and it creates opportunities for engineers to review the architecture and remove unnecessary components.</p>"},{"location":"architecture/architecting-for-simplicity/#continuous-evaluation","title":"Continuous Evaluation","text":"<p>Architecture is not a one-time thing; it is an ongoing process. As the system evolves and new features are added, it is possible that the system might deviate from the original design. If changes are made without considering the existing architecture, it can lead to a divergence from the original design and potentially introduce technical debt and other issues, as architecture is a set of rules to make the software work within a reasonable scope. When the scope of the system grows beyond the current architecture\u2019s capabilities, it may be necessary to introduce a new architecture that can handle the increased complexity and demands. Therefore, it\u2019s important to continuously evaluate the architecture and make necessary changes or even completely re-designed it to ensure it remains scalable, flexible, and maintainable.</p>"},{"location":"architecture/architecting-for-simplicity/#conclusion","title":"Conclusion","text":"<p>To establish a strong foundation for extending features and maintaining the system, we prioritize achieving architectural simplicity. This approach guarantees that the team understands the system\u2019s architecture, enabling them to handle any arising issues and preventing the need to deal with unknowns.</p> <p>Moreover, simplifying the architecture allows us to easily integrate new requirements and expand the system\u2019s capabilities without resolving any unknowns or seeking input from other engineers to comprehend the current architecture. By prioritizing architectural simplicity, we also promote increased engineering efficiency and architecture reliability.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/architecture-mystery/","title":"Architecture Mystery","text":"<p>Working as a software engineer, you might always hear people talking the word \u201carchitecture\u201d. But, what is architecture? I think the first word that comes into your mind is microservices architecture.</p> <p>Of course, microservice is an architectural pattern. But it is too vague. When talking about architecture, usually, we can refer to different granularities and layers, roughly including:</p> <ul> <li>System</li> <li>Subsystem</li> <li>Module</li> <li>Function</li> </ul> <p>With these layers in mind, we can describe architecture from different angles. The example below explains the architecture of a supply chain system in different granularities.</p> <p></p>"},{"location":"architecture/architecture-mystery/#system","title":"System","text":"<p>System level architecture describes the system as a whole. At this level, we can see all subsystems are listed and their dependencies. For example, a supply chain system might have an API Platform, Supply Chain Visualization, User, Transaction, Trade Item, and so on. All these subsystems together form the supply chain system.</p>"},{"location":"architecture/architecture-mystery/#subsystem","title":"Subsystem","text":"<p>Subsystem level architecture describes the subsystem capabilities and what modules the system provided. As you can see, System and subsystem levels are quite similar. The difference between them is the granularity.</p>"},{"location":"architecture/architecture-mystery/#module","title":"Module","text":"<p>Module level describes the actual functionalities that the subsystem provided.</p>"},{"location":"architecture/architecture-mystery/#function","title":"Function","text":"<p>Function level describes the actual code implementation for the functions.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/boost-api-read-performance-by-10x-with-cache/","title":"Boose API Read Performance by 10x with Cache","text":"<p>Today, we are going to discuss applying a cache strategy to the API in order to boost the API read performance.</p> <p>In a normal retrieval API call, we have 4 stages frontend, API, service, and eventually reach the database. This is how it looks:</p> <p></p> <p>The system might have a bottleneck on the database layer if the traffic is large. Under this architecture, all traffics hit the database directly and it can cause resource contention issues, especially if there are a large number of concurrent requests accessing the database.</p> <p>To address this issue, we can consider using a caching technique \u2014 Cache-aside Pattern. It is a popular cache strategy in software engineering. The app always read data from the cache first and then reads from the database if the cache is missed and writes back to the cache.</p> <p></p> <p>By introducing a new architecture that includes caching, an application can benefit from faster data access because the cache is built on top of memory storage.</p> <p>Memory storage is much faster than disk storage, which is typically used for storing data in a database. When data is stored in memory, it can be accessed and retrieved much faster than if it were stored on a disk.</p> <p>By using caching, an API can improve its performance by reducing the response time for frequently accessed data. This can lead to a better user experience and can also help reduce the load on the database.</p> <p>To use Cache-aside Pattern, we should consider:</p> <ul> <li> <p>Percentage of Read &amp; Write - Consider the percentage of read and write operations in the API. If the API has a higher percentage of reads, caching can be more effective in improving performance. If the API has a higher percentage of writes, caching may require additional considerations for maintaining consistency between the cache and the database.</p> </li> <li> <p>Total Traffic - Consider the total traffic to the API and the database. Caching can help reduce the load on the database, but if the traffic is too low, you may not need a cache, as it will increase the architecture complexity.</p> </li> <li> <p>Consistency - Maintaining consistency between the cache and the database is important to ensure that the data being used by the API is accurate and up-to-date. If the API needs high consistency, you might need to think deeper about the trade-off of applying a cache strategy to the API.</p> </li> <li> <p>Data Characteristic - Consider the data traffic and characteristic. It may not be effective if the traffic to a particular data is too high. You may need to implement additional scaling strategies, such as sharding or load balancing to handle the hot data.</p> </li> </ul> <p>In conclusion, when implementing the cache-aside pattern, you should consider the percentage of read and write operations in the API, the total traffic to the API and database, and consistency between the cache and the data source. These factors will help you create a better caching strategy for improved performance, reduced load on the database, and data consistency.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/cache-delete-strategy/","title":"Cache Delete Strategy of Cache-aside Pattern","text":"<p>In the cache patterns post, I asked a question:</p> <p>Do you know why we don\u2019t remove the cache key first and write to the database afterward?</p> <p>Here is the reason why we don\u2019t do this.</p>"},{"location":"architecture/cache-delete-strategy/#delete-the-cache-before-updating-the-database","title":"\u274c Delete the cache before updating the database:","text":"<p>According to the diagram above, it is obviously a dirty read that happened in steps 4 to 6. As you can see, we perform step 3 to delete the cache and now a Read request comes and found that the cache is missing. So, it performs an action to read the data from the database and set the data back to the cache.</p> <p>Since the Write request is not finished yet and the Read request runs faster than the Write request so a data inconsistency problem happens in the cache. The subsequent requests will be reading the staled data from the cache since there is no action to remove the cache until the next Write.</p> <p>To mitigate the data inconsistency problem, we can update the workflow to the following sequence.</p>"},{"location":"architecture/cache-delete-strategy/#delete-the-cache-after-updating-the-database","title":"\u2705 Delete the cache after updating the database:","text":"<p>Now, the workflow is updated. As you can see in the diagram above, we perform a database update first and clear the cache afterward. </p> <p>Under this flow, the Write request can delete the cache even if the Read request set the stale data into the cache. The subsequent requests will be reading the correct data from the cache. But, there is still a problem in steps 2 to 5 which is related to the execution sequence. </p> <p>If step 5 runs faster than step 4 (set cache), it will cause the data inconsistency problem again. But, it is less chance to have this problem in most of the use cases.</p> <p>Also, when adopting a cache strategy, we always need to have a reasonable expiry time to deal with the data inconsistency problem. We are not designing a strongly consistent system, so a little bit of data inconsistency is acceptable in most of the use cases.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/consistency-vs-availability/","title":"Consistency and Availability","text":"<p>When talking about Consistency and Availability, data replication is one of the topics that we can discuss.</p> <p>Data replication is one of the ways to increase availability. The more you replicate, the higher availability. Conversely, the more you replicate, the less consistent. </p> <p>In most systems, we prefer availability over consistency. If the system is not available, then no matter how consistent it is, people will not use it. Instead, if the system is highly available with a good user experience, even if we sacrifice a little bit of data consistency, people will still love it.</p> <p>To further discuss this topic, let\u2019s take the following scenario as an example. Imagine we are working on a heavy-read system. Now, A service is the data provider for B, C, D, and E services.</p>"},{"location":"architecture/consistency-vs-availability/#embrace-consistency","title":"Embrace Consistency","text":"<p>In a consistency system, every time when we read A service\u2019s data in B, C, D, and E services, they need to invoke A service to get the necessary data.</p> <p>What if now A service is down? We can\u2019t read the necessary data anymore and B, C, D, and E services are all gone too.</p> <p>If we change the design a little bit, the output will be totally different.</p>"},{"location":"architecture/consistency-vs-availability/#embrace-availability","title":"Embrace Availability","text":"<p>In this design, we get the necessary data from A service first when writing data for B, C, D, and E services. </p> <p>Now, moving to read data scenario. Since we already have a copy of A\u2019s data, B, C, D, and E services do not need to invoke A service to get the data anymore.</p> <p>What if A service is down? It will only affect the writing data scenario but the reading data scenario will still be functioning. In a heavy-read system, this design helps a lot. When reading data, we don\u2019t need to depend on the data provider. Also, A service does not have to handle the read traffic from B, C, D, and E services.</p>"},{"location":"architecture/consistency-vs-availability/#whats-the-trade-off-for-this-design","title":"What\u2019s the trade-off for this design?","text":"<ul> <li>More storage is needed for the data copy</li> <li>Data inconsistent when the data in A service is updated</li> </ul> <p>Compared to the impact that the first design bring to us, the trade-off of the second design seems reasonable and we can tolerate it in most scenarios.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/design-for-adaptability/","title":"Design for Adaptability","text":"<p>Designing software architecture requires careful consideration of various factors to ensure its effectiveness. In this post, I will share some ideas for creating a software architecture that can adapt to different conditions and resources.</p> <p>When designing architecture, there are several important aspects to consider:</p>"},{"location":"architecture/design-for-adaptability/#team-resources","title":"Team Resources","text":"<p>It is crucial to take into account the resources available within the team. Even with great architectural ideas, lack of resources can prevent successful execution. For example, a small team cannot build something as complex as Google. Therefore, the architecture design should align with the current team resources and the knowledge level of the engineers involved.</p>"},{"location":"architecture/design-for-adaptability/#time-to-market-ttm","title":"Time-to-Market (TTM)","text":"<p>The architecture design should be mindful of the overall development time required to bring the product to market. If there are strict time constraints, it would not make sense to implement an architecture that requires significantly more time than available. TTM is a critical factor as it directly impacts revenue generation. If an architecture cannot adapt to this constraint, it should not be considered.</p>"},{"location":"architecture/design-for-adaptability/#business-adaptation","title":"Business Adaptation","text":"<p>The architecture design should be aligned with the current state of the business. For instance, companies like Airbnb started with a monolithic application and later evolved into microservices and a combination of micro and macroservices. By considering the business scope, the architecture can be tailored to meet the specific needs of the company. Designing an architecture for a significantly higher workload than the current demand would result in unnecessary investment and inefficiencies.</p> <p>Airbnb Architecture Evolution:</p> <ul> <li>Monolith (2008 - 2017)</li> <li>Microservices (2017 - 2020)</li> <li>Micro + Macroservices (2020 - Present)</li> </ul> <p>Ref: https://www.infoq.com/presentations/airbnb-culture-soa/</p> <p>The evolution of Airbnb\u2019s architecture exemplifies how a large tech company adapts its architecture to align with its evolving business needs. As the business grows, the architecture evolves accordingly. This demonstrates the importance of periodically reassessing the existing architecture, removing outdated elements, and adapting it to meet new requirements.</p>"},{"location":"architecture/design-for-adaptability/#conclusion","title":"Conclusion","text":"<p>Creating an excellent architecture begins with establishing a solid foundation. While it\u2019s important to plan for future needs, it\u2019s equally essential to invest time wisely and prioritize current business demands. Predicting the future is challenging, and dedicating significant resources to shaping the architecture for hypothetical scenarios may not be efficient.</p> <p>One might argue that designing for future scalability is necessary in order to become a successful company. While this is true, it\u2019s important to balance long-term goals with current priorities. The architecture can be designed with future scalability in mind, but implementing it immediately without actual business needs may not be the best approach. In a fast-growing company, it\u2019s crucial to allocate resources effectively and adapt the architecture when the business demands it.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/event-driven-payment-architecture/","title":"Payment Event-driven Architecture","text":"<p>Event Driven Design is a popular architectural style and has many benefits if you are building a complex system, especially building a state transitions workflow.</p> <p>Let\u2019s take the payment order workflow as an example. An order has many states like Pending, Assigned, Processing, Failed, Paid, and Completed. When the order is placed in the system, it will be Pending. After that, the order will be Assigned to one of the payment channels. The status will become Processing when the payment request sends to a third-party gateway. It will be either Failed or successfully Paid when the third-party gateway returns the result to us. Finally, we need to send a callback to our client to complete the order, it will be Completed at this stage.</p> <p></p> <p>With the requirements above, we can break down the problem into different parts.</p> <ol> <li>Handling order creation</li> <li>Assigning payment channel to new orders</li> <li>Sending payment requests to third-party vendors</li> <li>Updating order status and making sure the status sequence</li> <li>Sending callbacks to clients</li> </ol> <p>Okay, so how do we utilize Event Driven Design to help us archive the above goals?</p> <p>Before talking about Event Driven Design, let\u2019s think about the request-response pattern first. Probably it will be something like the one below. Let Order Service orchestrate the order workflow and use direct HTTP calls to invoke other services. (Or Order Service invokes Payment Service, and Payment Service invokes Payment Channel Allocation Service.)</p> <p></p> <p>When the payment provider callback the Payment Service, it needs to invoke Order Service to update the status.</p> <p>What are the problems in this architecture? Think about it. Leave your thoughts in the comment section.</p> <p>With the event-driven design, the architecture might look like the below diagram:</p> <p></p> <p>Introduce different event topics in the architecture, like order-created for notifying other subscribers that a new order is available in the system. Once subscribers receive a new order created event from the topic, they can do their own job accordingly. For example, the Payment Gateway Allocation Service might need to allocate a payment channel to a new order. </p> <p>After that, the Payment Gateway Allocation Service might further publish another event payment-channel-allocated to tell its subscribers that a new order with an allocated payment channel is now available to consume. The Payment Service will then fetch the allocated order from the topic and consume it.</p> <p>When Payment Service sends the order to third-party providers, it will publish an order-updated event to tell subscribers a payment order is updated.</p> <p>In this workflow, the Order Service does not need to talk to the Payment Gateway Allocation Service directly and the Payment Gateway Allocation Service does not need to talk to the Payment Service too. Instead, they communicate with each order through events.</p> <p>In this architecture, when a new payment order is submitted to the system, it is like telling everyone there is a new event about order creation, if you care about order creation, please do your own handling. That said, the upstream services do not need to care about how the downstream service handles the logic after order creation.</p> <p>What are the benefits of using this architecture?</p> <ul> <li>Decouple service dependency</li> <li>Better availability and scalability</li> </ul> <p>Can you tell me any drawbacks of event-driven design? Leave comments below to share your thoughts.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/exploring-architetcure/","title":"Exploring Architetcure: What Does It Really Mean?","text":"<p>I have been delving into architecture across many discussions. But what exactly does architecture mean? While many might perceive architecture as the utilization of technologies to address engineering challenges \u2013 which is indeed one of its core objectives \u2013 it includes more than just that. It serves as a guiding framework at various project stages, beyond engineering hurdles.</p> <p></p> <p>The underlying purpose of architecture extends to the identification of intricacies, risk assessment, communication, and the strategic allocation and integration of resources.</p>"},{"location":"architecture/exploring-architetcure/#integration-and-allocation-of-resources","title":"Integration and Allocation of Resources","text":"<p>A pivotal consideration in architectural decisions is an awareness of existing engineering resources. Resource availability determines whether the team possesses sufficient engineering capacities to facilitate a timely feature rollout, thereby impacting time-to-market. During the architectural phase, it becomes imperative to measure the necessary engineering resources required and assess their value proposition. Given the precious nature of engineering resources, strategic investment in the right areas is crucial to enhance product value. If a feature holds substantial significance, a proportional augmentation of resources can be allocated. This necessitates the seamless integration and judicious allocation of all available resources.</p>"},{"location":"architecture/exploring-architetcure/#management-of-risk","title":"Management of Risk","text":"<p>Once the resource allocation is outlined, risk evaluation becomes paramount. The architecture design phase mandates a comprehensive assessment of potential risks, necessitating trade-offs and decisions to be made. The acceptability of these risks upon deployment becomes a vital consideration. By effectively mitigating explicit and critical risks, it becomes feasible to control the impact of challenges that may exceed the architecture\u2019s capabilities.</p>"},{"location":"architecture/exploring-architetcure/#control-of-complexity","title":"Control of Complexity","text":"<p>A fundamental facet of architecture pertains to complexity management. Each design inherently carries a level of complexity, but not all levels are tenable for a given team. During the architectural blueprinting, it becomes imperative to account for the team\u2019s proficiency and resource availability. This strategic consideration aids in maintaining a complexity threshold that aligns with the team\u2019s capabilities, fostering a sustainable architecture. Architecture is not a universal solution but varies depending on the situation. To create an effective solution, we should eliminate unnecessary complexity and create a design to address the specific problem we intend to solve.</p>"},{"location":"architecture/exploring-architetcure/#communication-and-collaboration","title":"Communication and Collaboration","text":"<p>Architecture acts as a communication tool connecting team members, stakeholders, and various technical and non-technical groups engaged in the project. A clearly outlined design document plays a crucial role in ensuring that everyone comprehends the project\u2019s framework and objectives.</p>"},{"location":"architecture/exploring-architetcure/#conclusion","title":"Conclusion","text":"<p>In a nutshell, architecture\u2019s purpose extends beyond engineering challenges. It aids in resource optimization, risk mitigation, complexity control, and effective communication, making it a pivotal factor in successful project development.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/handling-job-efficiently/","title":"Handling Jobs Efficiently","text":"<p>Today, I\u2019ll be covering a topic about background tasks in software development. It is so common that software engineers need to write code to trigger something automatically in different time intervals.</p> <p>Working in a backend system, you might need to deal with these scenarios:</p> <ul> <li>Cancel the order if payment is overdue</li> <li>Retry with delay</li> <li>Cancel application submission if supporting documents haven\u2019t upload</li> <li>\u2026</li> </ul> <p>Imagine if you are working on an eCommerce system, customers are able to place orders in the system and make the payment. Also, the payment must be made within 20 mins the order being created or else cancel the order.</p> <p>How will you handle this requirement? Let\u2019s think about it.</p> <p>I guess the first thing that comes to your mind is the Scheduled Job. Using scheduled jobs, the app can run a task periodically in the background.</p>"},{"location":"architecture/handling-job-efficiently/#scheduled-job","title":"Scheduled Job","text":"<p>When the order is placed in the database, a scheduled job will be in and scan the database periodically say every few seconds or more. If there is any order that matches the criteria (creation time after 20 mins and unpaid records), the scheduled job will cancel those orders.</p> <p>Everything looks good by using this approach. It solves the problem that describes in the requirement. But, let\u2019s think deeper.</p> <p>How do we define the frequency of the job? It\u2019s hard. We can\u2019t make the interval too frequent or too long.</p> <p>What if the data size is large? Probably will be very slow.</p> <p>What will be happened if we scan this large dataset frequently? Probably adds a lot of pressure to the database.</p> <p>What if there is no new order being made and we still keep scanning the table? Probably we are wasting resources.</p> <p>To mitigate this problem, Delay Queue comes into play. We can introduce a delay queue to the system to solve the problems mentioned above.</p>"},{"location":"architecture/handling-job-efficiently/#delay-queue","title":"Delay Queue","text":"<p>With a delay queue, we can put the created order into the queue with 20 mins delay time. A job handler will be listening to the queue to see if any order is available to consume. If now the order has been created for 20 mins, the job handler will get this order from the queue and search the order status in the database.</p> <p>There are 2 possible cases here:</p> <ul> <li>Payment is already made, then ignore the order canceling flow.</li> <li>Payment hasn\u2019t been made, then run the order canceling flow.</li> </ul> <p>Now, we don\u2019t need to scan the database frequently, especially in the zero new orders scenario. Also, we can get the order by primary key. Using the delay queue helps mitigate the pressure on the database.</p> <p>Possible delay queue solutions:</p> <ul> <li>Redis</li> <li>RabbitMQ</li> <li>AWS SQS</li> </ul> <p>To sum up, for small-scale systems, the scheduled job is definitely a good and simple solution. But, when the data and traffic get more and more, the delay queue will be more suitable to solve the problem.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/how-does-lalamove-scale-its-communication-platform%3F/","title":"How Does Lalamove Scale Its Communication Platform?","text":""},{"location":"architecture/how-does-lalamove-scale-its-communication-platform%3F/#introduction","title":"Introduction","text":"<p>The communication platform acts as an important role in the Lalamove ecosystem in communicating with Lalamove\u2019s users through various communication channels. Lalamove sends out hundreds of millions of communication monthly to communicate with Lalamove\u2019s users through various communication channels such as SMS, Email, and Push. Our goal is to provide a scalable, state-of-the-art, and cost-effective omnichannel communication platform to boost engineering and operational efficiency for our internal teams.</p> <p>However, we have identified several challenges from both business and engineering viewpoints within our current communication platform. To gain a better understanding of these challenges, it is necessary to examine our legacy architecture.</p>"},{"location":"architecture/how-does-lalamove-scale-its-communication-platform%3F/#challenge","title":"Challenge","text":"<p>Under this architecture, each feature team directly connected their services to the communication services, creating scalability challenges and various difficulties for Lalamove\u2019s engineering team.</p> <p>The following are the challenges we faced with this architecture:</p> <ul> <li> <p>Difficult to integrate with multiple communication services - Feature teams have to integrate with multiple communication channels independently when sending communication. For instance, if there are five communication channels, the feature teams have to integrate with the communication platform five times, which requires significant integration and communication effort and adversely impacts engineering efficiency.</p> </li> <li> <p>Lack of communication orchestration - The legacy architecture is a multi-channel solution but lacks omnichannel capability. This makes the communication platform not flexible enough for feature teams.</p> </li> <li> <p>Redundant development effort - Currently, the development effort for common communication features is huge under this architecture such as data visibility and template management. We need to develop the same feature in multiple communication channels independently. This hurts engineering efficiency and maintainability.</p> </li> </ul> <p>To improve Lalamove\u2019s communication capability, we developed a communication gateway. In the following discussion, we will introduce the new architecture of the communication platform and highlight the various capabilities that the communication gateway brings to our internal users.</p>"},{"location":"architecture/how-does-lalamove-scale-its-communication-platform%3F/#communication-platform-architecture","title":"Communication Platform Architecture","text":"<p>The communication gateway is a critical component of the communication platform, allowing feature teams to integrate once and enjoy seamless updates.</p> <p>In this architecture, we aim to provide the following capabilities:</p> <ul> <li>Provide scalable APIs and communication orchestration to enable omnichannel communication</li> <li>Encapsulate communication logic to avoid duplicated effort</li> <li>Define clear responsibilities for different stakeholders by separation of concerns</li> <li>Enable business improvement through data transparency</li> </ul> <p>By rearchitecting the communication platform, we redefined the core values of the communication platform. It is important to understand our project\u2019s core values, as it defines the vision of the communication platform and reflects our philosophy.</p>"},{"location":"architecture/how-does-lalamove-scale-its-communication-platform%3F/#core-value","title":"Core Value","text":"<p>The next-generation communication platform is built to provide four key capabilities: Ownership, Scalability, Flexibility, and Transparency, together they form the core values of the communication platform and enable the communication platform to provide a complete solution for internal users.</p> <p></p>"},{"location":"architecture/how-does-lalamove-scale-its-communication-platform%3F/#clear-ownership","title":"Clear Ownership","text":"<p>Clear ownership is provided in the communication platform, where each template is assigned to a specific team. This enables us to easily identify the responsible team and the triggering service in case of any production issues. Separation of concerns is also prioritized in Lalamove\u2019s communication ecosystem, with the business team owning the communication content rather than the tech team.</p>"},{"location":"architecture/how-does-lalamove-scale-its-communication-platform%3F/#scalability","title":"Scalability","text":"<p>We simplify integration with the communication platform by offering an omnichannel API. The communication gateway abstracts the communication process, making it easy for users to leverage its benefits. By integrating with the communication platform once, users can enjoy seamless updates.</p>"},{"location":"architecture/how-does-lalamove-scale-its-communication-platform%3F/#flexibility","title":"Flexibility","text":"<p>To effectively manage communication under various conditions such as channel switching and content management, we need a flexible approach. The communication platform offers just that, allowing our business teams to update channels and content without needing our engineers to make any code changes or perform deployments. This not only simplifies the management of communication channels but also enables our platform to be more flexible.</p>"},{"location":"architecture/how-does-lalamove-scale-its-communication-platform%3F/#transparency","title":"Transparency","text":"<p>Our goal with the next-generation communication platform is to provide our internal users with data transparency by allowing them to see the effectiveness of our communication efforts. To achieve this, we have a data pipeline to ingest the data and provide data transparency to our internal users.</p> <p>Then, how do we rearchitect the communication platform to solve the challenges that we had and achieve our core values?</p>"},{"location":"architecture/how-does-lalamove-scale-its-communication-platform%3F/#omnichannel","title":"Omnichannel","text":"<p>With the omnichannel API, the communication gateway is able to orchestrate communication in different communication channels according to different business strategies. Also, we provide a single entry point for our users to allow them to integrate with the communication platform once and enjoy seamless updates.</p> <p>In this design, we have a concept called \u201cAction\u201d and it is used to define communication. An \u201cAction\u201d usually refers to a business scenario. Each \u201cAction\u201d maps to one or many communication channels. By using this concept, we can scale our communication easier without coupling communication to a specific communication channel.</p> <p>This brings flexibility and reliability to the communication platform and convenience to the users. The users can switch the channels anytime without code changes and get rid of integrating the communication platform repeatedly.</p>"},{"location":"architecture/how-does-lalamove-scale-its-communication-platform%3F/#communication-encapsulation","title":"Communication Encapsulation","text":"<p>Communication encapsulation is important, as it helps to centralize communication handling logic. One of the principles we adopted in the communication platform is the Separation of Concerns.</p> <p>To understand it, we can take the content composition as an example. The content and channel are managed by the communication platform instead of our consumers.</p> <p>That being said, users can register a template in the communication platform and send a communication request by providing the necessary parameters. After that, all the content composition logic will be done by the communication platform. This separates the concerns of different stakeholders and makes communication logic more cohesive.</p>"},{"location":"architecture/how-does-lalamove-scale-its-communication-platform%3F/#template-portal","title":"Template Portal","text":"<p>The template portal serves as a user interface that enables internal users to manage communication templates and channels in a self-service manner, without relying on the tech team to change backend code. With the separation of concerns, we streamline the process of managing communication template translations and provide autonomy to internal users.</p>"},{"location":"architecture/how-does-lalamove-scale-its-communication-platform%3F/#key-takeaway","title":"Key Takeaway","text":"<p>There are a few key takeaways from our re-architecture journey, including</p> <ul> <li> <p>Content Management Workflow - Content management was a critical challenge at Lalamove as the content was spread across various services, making it difficult to scale and define maintenance responsibility. To address this challenge, we aligned our core values and defined that the content should be managed by business users rather than software engineers. To achieve this, we developed a portal that empowers business users to manage template content and have communication autonomy.</p> </li> <li> <p>Communication Orchestration - Communication orchestration is a critical aspect of any communication platform, as it determines whether the platform can be easily scaled or not. At Lalamove, we faced the challenge of having multiple communication channels but lacking omnichannel capability. Through our experience, we have come to understand the importance of having a communication platform that is equipped with omnichannel capability, as it can provide the necessary flexibility and reliability to ensure seamless communication across all channels.</p> </li> <li> <p>Importance of Microservice Orchestration - The orchestrator service, Communication Gateway, in our new architecture encapsulates and centralizes common communication logic, which eliminates the need for multiple entry points and reduces the complexity of the communication platform. Communication services can now focus solely on communication, while the communication gateway handles communication orchestration, such as request management, channel selection, and content composition. This design not only improves engineering efficiency but also allows for easier maintenance and scalability of the communication platform.</p> </li> </ul>"},{"location":"architecture/how-does-lalamove-scale-its-communication-platform%3F/#conclusion","title":"Conclusion","text":"<p>In the legacy architecture, there were independent components with no connection between them, leading to challenges.</p> <p>With this in mind, we spent so much effort thinking about how to connect different components together and build a scalable, state-of-the-art, and cost-effective omnichannel communication platform. It offers numerous benefits such as template and channel management, scalable APIs, and communication orchestration.</p> <p>The communication platform is designed to provide 4 key capabilities including Ownership, Scalability, Flexibility, and Transparency, and they combine to establish the Communication Platform\u2019s core values. This help improves the developer experience and boosts engineering and operational efficiency at Lalamove.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/log-monitoring-and-alerting/","title":"Log Monitoring and Alerting","text":"<p>In this post, we are going to talk about log monitoring and alerting. It is also an important component of a system. Without this, your software engineers do not know what is happening in the whole system. That said, when an error occurs, no one knows and the engineering team does not know whether the system performs well or not. After reading this post, I expect you will know how a log monitoring system works.</p> <p>Here we are going to have a high-level understanding of a log monitoring system. There are many solutions for log monitoring and alerting like New Relic, Datadog, etc. In this post, I am gonna give you an idea for log monitoring and alerting on leveraging elastic stack.</p> <p></p>"},{"location":"architecture/log-monitoring-and-alerting/#collect-and-process-logs","title":"Collect and Process Logs","text":"<p>For collecting logs from containers, we can use the fluent bit to read the container logs directly and forward the logs to the fluentd. After that, the fluentd will parse and aggregate the logs according to your rules and then push the logs to the elasticsearch for storage.</p>"},{"location":"architecture/log-monitoring-and-alerting/#send-alerts","title":"Send Alerts","text":"<p>After storing the logs, we want to leverage the logs for alerting if any error occurs by using ElastAlert. It will search the logs on the elasticsearch according to the rules that you defined. If any record hits the condition, ElastAlert will push the notification to Slack(or other channels) to notify your engineering team or business team.</p>"},{"location":"architecture/log-monitoring-and-alerting/#query-logs","title":"Query Logs","text":"<p>Kibana is introduced in this design to solve the logs visualization and query issue. Your team can browse the logs from Kibana UI and filter the results by whatever condition you want.</p>"},{"location":"architecture/log-monitoring-and-alerting/#monitoring-dashboards","title":"Monitoring Dashboards","text":"<p>Kibana is a great tool for you to create some metrics based on your logs. It enables your team to create graphs, aggregated data, tables, etc. You can create the dashboards based on your business need including system performance, daily error rate, traffic, and so on.</p>"},{"location":"architecture/log-monitoring-and-alerting/#conclusion","title":"Conclusion","text":"<p>In conclusion, a log monitoring system at least needs to have alerts, dashboards, and good query functions. Also, your logging system needs to be scalable to support a huge amount of log entries. For this, Fluentd and Elastic Stack both provide good scalability for users to scale the instances.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/mock-service/","title":"Redeisn Your Development Workflow with Mock Service","text":"<p>Today, I will be sharing the idea of a Mock Service in the software development workflow.</p> <p>It is common that a system to integrate with third-party providers such as payment and communication providers. In normal architecture, a service connects to a provider directly in different environments.</p> <p></p>"},{"location":"architecture/mock-service/#what-are-the-problems-in-this-architecture","title":"What are the problems in this architecture?","text":"<ul> <li> <p>Unable to test the error use cases from providers - Your app cannot test unexpected responses under this architecture because you do not have the right to control the third-party provider.</p> </li> <li> <p>Unable to have end-to-end automated tests - Your app does not have the capability to test different behaviors with a real provider.</p> </li> </ul> <p>To tackle the listed problems, a mock provider is introduced into the architecture.</p> <p></p> <p>This approach allows developers to test and verify the app\u2019s behavior in a controlled environment before deploying it to the production environment. The mock provider is designed to respond to requests in the same way as a real provider would, allowing developers to test error cases and edge cases that might not occur during normal operation. By simulating different scenarios, developers can ensure that the app behaves as expected under different conditions, reducing the risk of bugs and issues in production.</p>"},{"location":"architecture/mock-service/#what-benefits-does-this-architecture-bring-to-you","title":"What benefits does this architecture bring to you?","text":"<ul> <li> <p>Empower developers to test different behaviors - With a mock provider, we can create our own test assets to mock different behaviors, and that enables developers or QA to run a complete test for different scenarios before rolling out.</p> </li> <li> <p>Enable end-to-end automated tests - A mock provider can facilitate end-to-end automated tests to ensure the latest code does not break the existing behaviors. We can control the percentage of success or failure rate to cover different scenarios. With end-to-end tests, we can promote refactoring culture in the team and help developers build confidence in shipping code. Also, if there is anything broken, we can send an alert to notify developers.</p> </li> <li> <p>Simulate Chaos - With the mock service, we can randomly inject chaos into the mock provider to verify whether our services can handle the chaos properly.</p> </li> </ul>"},{"location":"architecture/mock-service/#conclusion","title":"Conclusion","text":"<p>To sum up, integrating with third-party providers in a system is common and it might have different problems during the integration such as hard to simulate unexpected behaviors. By introducing a mock provider to the system architecture, developers can verify the API behaviors in a controlled environment, test different scenarios and error cases, and reduce the dependencies on third-party providers. With that, developers and QA can test whatever they want by simulating the behaviors of a real provider.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/pubsub-practical-guide-trade-off/","title":"Pub/Sub Practical Guide: Trade-off","text":"<p>Pub/Sub makes an architecture to be resilient to failures and increases system availability in a high-traffic situation. But, it also brings us several drawbacks. In this article, I will walk you through the potential drawbacks of Pub/Sub architecture.</p> <p>Applying Pub/Sub architecture to a system might have the following trade-offs:</p>"},{"location":"architecture/pubsub-practical-guide-trade-off/#latency","title":"Latency","text":"<p>Introducing a Pub/Sub to a system will increase latency. The diagram shows the difference between direct connection and indirect connection. Originally, Service 1 only communicate with Service 2 directly through REST or RPC. After applying Pub/Sub, the request is submitted to Message Broker first, and then wait for Service 2 to consume the messages. This introduces latency to the system and it will be obvious when traffic is high.</p>"},{"location":"architecture/pubsub-practical-guide-trade-off/#development-complexity","title":"Development Complexity","text":"<p>Before applying Pub/Sub architecture, the development complexity between 2 services is HTTP request where HTTP client is common in any architecture, so we can ignore the complexity. Now, after introducing Pub/Sub to the system, we bring extra complexity to the architecture. Not every developer is familiar with Pub/Sub architecture in-depth. If there are any issues related to the Pub/Sub architecture, does anyone in the team know how to solve them? If not, it will bring unknowns to the architecture.</p>"},{"location":"architecture/pubsub-practical-guide-trade-off/#devops-complexity","title":"DevOps Complexity","text":"<p>Bringing Pub/Sub to the system architecture also increase DevOps complexity. DevOps engineers need to manage the complexity that the Pub/Sub brings to the architecture including creating a cluster and managing topics and partitions. Without knowing the technology in-depth, it is hard to manage the complexity reasonably.</p>"},{"location":"architecture/pubsub-practical-guide-trade-off/#conclusion","title":"Conclusion","text":"<p>To sum up, Pub/Sub is good but we still need to manage the complexity. By knowing the trade-off, we can make better decisions when creating an architecture. In general, applying Pub/Sub to architecture will bring latency, development complexity, and DevOps complexity to us, apply it wisely and consider the trade-offs to see if the team can pay for it.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/pubsub-practical-guide-use-case/","title":"Pub/Sub Practical Guide: Use Case","text":"<p>The Pub/Sub pattern is widely used in large complex architecture and serves different purposes. In this article, I will summarize the scenarios in that a message queue can be applied.</p>"},{"location":"architecture/pubsub-practical-guide-use-case/#decouple-service-dependency-and-asynchronous","title":"Decouple service dependency and Asynchronous","text":"<p>To understand how Pub/Sub architecture benefit to this scenario, it is necessary to study a little about the architecture without Pub/Sub.</p> <p></p> <p>In this architecture, Order Service is tightly coupled with the other 3 services by invoking APIs. It causes 3 problems:</p> <ul> <li>Order Service needs to have a code change if now we don\u2019t need to depend on Statistics Service</li> <li>The other 3 services\u2019 response times are all accumulated to the Order Service response time</li> <li>Order Service needs to handle service unavailable issues for its dependencies</li> </ul> <p>Now, how does Pub/Sub solve these problems?</p> <p></p> <p>We change the architecture to include Pub/Sub. Now, the Order Service will always publish an event if there is a new order. Interested parties subscribe to the topic and handle the event with their own logic.</p> <p>If the statistics service no longer cares about order creation, it can remove the subscription and the order service does not change any code.</p> <p>Since it is asynchronous now, the response times from other services will not accumulate to the Order Service and also the Order Service no longer needs to handle service unavailable issues for its dependencies, instead, it will be handled by the Pub/Sub.</p> <p>Can you guess what are the problems here after we adopt Pub/Sub architecture? Leave your thoughts in the comment below</p>"},{"location":"architecture/pubsub-practical-guide-use-case/#avoid-service-being-overwhelmed","title":"Avoid service being overwhelmed","text":"<p>Pub/Sub can also avoid overwhelming a service. Imagine we have 5K requests per second at peak hours and the notification service can only process 3K requests per second. If we allow the service to process all requests simultaneously, it will overwhelm the service.</p> <p></p> <p>By introducing Pub/Sub, we can temporarily store the requests in the message broker and consume the messages later. With this design, we can also have a stable consumption rate and avoid service overwhelming. In off-peak hours, the traffic is low and we can consume message backlog steadily.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/understand-observability/","title":"Understand Observability","text":""},{"location":"architecture/understand-observability/#what-is-observability","title":"What is Observability?","text":"<p>Observability is used to describe your system state so the software engineers can use that information to understand the system and identify issues.</p> <p>In a microservices architecture, log and trace are the important components. It helps software engineers to understand the current system behaviors and debugging. It can also be used to observe the application\u2019s performance like the daily error rate of a specific core API, how people use your application, etc.</p> <p>Observability is mainly composed of the following 3 components: </p>"},{"location":"architecture/understand-observability/#logs","title":"Logs","text":"<p>A log is an event with some descriptive elements like timestamp, log id, trace id, message, severity level and etc. It tells people what is happening in a particular action. A log is commonly in Plaintext or Structured format:</p>"},{"location":"architecture/understand-observability/#plaintext","title":"Plaintext","text":"<p>It is quite common to be used for logging but it is not search-friendly.</p>"},{"location":"architecture/understand-observability/#structured","title":"Structured","text":"<p>In a good logging system, it is not only a message but contains other metadata like IP address, custom fields for different actions and etc. That said, we want to enrich the log with meaningful data and catch what we need in different actions. Stripe suggests a good approach called Canonical Log Line which can help us aggregate the logs into a single log line per action per service.</p>"},{"location":"architecture/understand-observability/#traces","title":"Traces","text":"<p>Trace is a series of events that represents the end-to-end journey starting from client initialize request to request end. With a microservices architecture, each event that happened in a single service is called Span. All spans connect together is called Trace. A trace can help you understand the system bottleneck, system break, service dependency, etc.</p>"},{"location":"architecture/understand-observability/#metrics","title":"Metrics","text":"<p>Metric is aggregated data within a time series including timestamp, value, and other meaningful data. It can be a graph to describe system performance, daily error rate, traffic, etc.</p> <p></p> <p>According to the diagram above, we can observe that a particular Trace is composed of many Spans. A trace can identify this whole series of actions that happened in the system and connect these spans together for better searching.</p> <p>Also, from this diagram, we can know the service dependency of a specific API endpoint from upstream to downstream.</p> <p>Apart from that, a Trace can also be used to measure the execution time between each service and visualize the request/event timeline from end to end. It helps software engineers to identify the system bottleneck.</p>"},{"location":"architecture/understand-observability/#conclusion","title":"Conclusion","text":"<p>In conclusion, Observability is a very important component of a system and it includes Logs, Traces, and Metrics. It helps software engineers to understand the system behaviors, performance, and debugging.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/understand-software-complexity/","title":"Understand Software Complexity","text":"<p>In software engineering, we always need to deal with complexity no matter it is designing architecture or writing code. Basically, complexity can be grouped into 2 types:</p> <ul> <li>Accidental Complexity</li> <li>Essential Complexity</li> </ul> <p>In the following sections, I will be explaining these 2 types of complexity. Okay, let\u2019s get started!\ud83d\ude0e</p>"},{"location":"architecture/understand-software-complexity/#accidental-complexity","title":"Accidental Complexity","text":"<p>This type of complexity is not from the original problem. It is just accidentally added to the solution when designing architecture or writing code. Simply put, this type of complexity is not necessary for solving the problem. It is from over-engineering or wrong design. That said, accidental complexity can be avoided by adopting the right design.</p>"},{"location":"architecture/understand-software-complexity/#essential-complexity","title":"Essential Complexity","text":"<p>This type of complexity is inherited from the problem. It is the essence of the problem that you need to resolve. That said, essential complexity cannot be avoided because it is the actual problem.</p>"},{"location":"architecture/understand-software-complexity/#case-study","title":"Case Study","text":"<p>Imagine the order service relying on many other services say 20 services. Assume a lot of requests come to the message service and it affects the order processing time.</p> <p>Also, we have multiple services that need to write databases and the database is slow because it cannot support that many requests. Now, you want to enhance the design to make order processing faster. Given that we accept latency for some non-important services. How would you solve this problem? </p> <p>Now, the problem is the API performance is not good enough, because we need to write data to different databases. It seems the database becomes the bottleneck of the application.</p> <p>Would you design a high-performance architecture that supports 10K+ transactions per second (TPS) in terms of making the API response faster?</p> <p>If you are going to do this, I would say you are making the wrong design/over-engineering the architecture. Basically, it is accidental complexity.</p> <p>The problem is more like a service coupling issue although we have multiple databases written here. Since the order service relies on many other services and the processing time from other services is accumulated to the order processing time. So, the essential complexity is the service coupling issue.</p> <p>To solve this issue, we can decouple the services and remove the unnecessary direct dependencies from the order service. For example, message service, statistic service, etc.</p> <p>By introducing a message queue, we can decouple the service dependencies and create a loosely coupled architecture.</p> <p>Now, we move the unnecessary operations out of the order workflow. When an order is placed, we publish a message to notify the subscribers. From the order service perspective, it does not care how the message/statistic service handles the order data. Instead, the message/statistic service needs to subscribe to the events that they are interested in and process them asynchronously.</p> <p>\ud83d\udca1 Do you know why do we prefer introducing a message queue against scaling the database and keep the current architecture? </p>"},{"location":"architecture/understand-software-complexity/#conclusion","title":"Conclusion","text":"<p>Let\u2019s wrap it up. I think when designing architecture, the most important thing is to understand the essence of the problem and dig into the problem and resolve it by using the right solution. Engineers should focus on the essential complexity and avoid accidental complexity. Understanding where the complexity from and what problems you are solving can definitely help you make the right decision when designing architecture.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/understand-system-bottleneck/","title":"Understand System Bottleneck","text":"<p>Do you know your system\u2019s bottleneck? This is the question that every software engineer needs to ask themselves. When designing a system, you always need to know where is the bottleneck of the system. Otherwise, you can\u2019t create a good system. When the problem comes, you don\u2019t know how to solve it or prevent the problems.</p>"},{"location":"architecture/understand-system-bottleneck/#what-is-a-bottleneck","title":"What is a bottleneck?","text":"<p>Basically, a bottleneck means if your system wants to process 1000 requests per second, but one of the components can only support 100 requests per second. Then that component will become the bottleneck of the system. Because that component limits the scalability of the system.</p> <p></p> <p>Imagine we have a six-lane road that can accept 6 cars in the road but in the middle, it becomes a two-lane road. Only 2 cars are allowed on the road. That said, the narrow road will become the bottleneck of the whole road. It might cause a traffic jam if the cars keep coming.</p> <p>In short, a bottleneck is that your system wants to process N requests but actually it cannot process that many.</p>"},{"location":"architecture/understand-system-bottleneck/#use-case","title":"Use Case","text":"<p>Do you know what are the bottlenecks of the below architecture?</p> <p></p> <p>I think most people will know there is a bottleneck between the frontend and backend. Let\u2019s say the backend can process 1000 requests per second but now the frontend sends 3000 requests per second. Definitely, the backend cannot undertake that many requests.</p>"},{"location":"architecture/understand-system-bottleneck/#what-can-we-do-to-mitigate-the-issue","title":"What can we do to mitigate the issue?","text":"<p>Scale the backend? Yes, we need to scale the backend first. If one backend can only support 1000 requests per second, then scale it to 3 backends or more.</p> <p>Do you think scaling the backend from 1 to 3 can solve our problem? It really depends. Why do I say that? Let\u2019s take a step back, do you know where is the bottleneck of the system?</p> <p>Is scaling the backend from 1 to 3 really can solve the problem? Scaling the backend to 3 does not mean the backend can process 3x traffic. For example, if the system bottleneck is not in the Backend application itself but in the DB or Payment Provider. Now, you scale the backend application is useless because that component is not the bottleneck.</p> <p>If the payment provider can only process 1000 requests per second, and it has a rate limit policy. Even if you scale the backend, the system still cannot process requests more than 1000 requests. Because the bottleneck is in the payment provider.</p> <p>When the problem comes, scaling the backend is easy because it is stateless, we can do horizontal scaling. But for stateful components like DB or external dependencies like payment providers, it is hard to scale when the online issue happens.</p>"},{"location":"architecture/understand-system-bottleneck/#conclusion","title":"Conclusion","text":"<p>Let\u2019s wrap it up here. When talking about bottleneck and scalability, we can\u2019t just focus on the app itself but also need to take care of other components like cache, database, external dependencies, etc.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/what-is-event-driven-architecture%3F/","title":"What is Event-driven Architecture?","text":"<p>Event-Driven Architecture is a software design pattern that helps to decouple service dependency. It uses a publish and subscribe model to connect different components together. To help us understand what is event-driven architecture better, let\u2019s take notification as an example.</p>"},{"location":"architecture/what-is-event-driven-architecture%3F/#context","title":"Context","text":"<p>Using direct communication, Ray needs to ask Ken and Mary to do different work like increasing SMS usage by 1 and sending SMS notifications. Also, Ray needs to wait for Ken and Mary to get back the result. In this scenario, Ray is like a coordinator to organize everyone\u2019s work. Imagine, if now Ken is going out to buy lunch and Ray needs to send an SMS. It is not possible to do because Ken is not available to help.</p> <p>How can we make it better using Event Driven Design? In event-driven design, Ray does not need to talk to Ken and Mary directly. Instead, Ray just needs to leave a message in the Slack SMS channel to tell everyone in the channel that a new SMS notification has been submitted. After that, Ken and Mary will do their own work when receiving a request from the Slack SMS channel.</p> <p>Let\u2019s convert it to a more technical diagram. Now, we have a notification service to receive users\u2019 notification requests.</p> <p></p>"},{"location":"architecture/what-is-event-driven-architecture%3F/#direct-communication","title":"Direct Communication","text":"<p>In direct communication, the notification service will ask the SMS service to send an SMS request directly via HTTP call. The SMS service will then send the notification to the external SMS provider and the notification service will need to wait for responses from the external vendor and the SMS service.</p>"},{"location":"architecture/what-is-event-driven-architecture%3F/#what-are-the-drawbacks","title":"What are the drawbacks?","text":"<ul> <li>Longer response time because notification service needs to wait for its downstream to return the result.</li> <li>If the SMS service is down, the requests will be lost.</li> </ul>"},{"location":"architecture/what-is-event-driven-architecture%3F/#event-driven-architecture","title":"Event-driven Architecture","text":"<p>With Event Driven Design, the notification service will not communicate with the SMS service directly. Instead, it will publish a notification message to the Event Broker.</p> <p>Now, how can the SMS service know there is an SMS request that needs to be handled? As I mentioned in the very beginning, the event-driven design uses a publish and subscribe model. What the SMS service needs to do is to subscribe to the SMS topic. Whenever the notification service publishes a new notification message to the SMS topic, the SMS service will then listen to the SMS topic to consume the messages from the Event Broker.</p> <p>Event Broker like Kafka has guaranteed the messages will be consumed at least once. If the message arrived at Event Broker successfully, it is guaranteed that the message will be consumed. So even if the SMS service is down, the messages are still in the event broker. Once the SMS service is back, it will start consuming again.</p>"},{"location":"architecture/what-is-event-driven-architecture%3F/#what-are-the-drawbacks_1","title":"What are the drawbacks?","text":"<p>Any drawbacks to event-driven architecture? Yes, for example, the architecture becomes complicated after introducing an event broker. The team might need to study how to use an event broker like Kafka. Also, if users want to know the result of the SMS service, we need to use other approaches. Because it is not using direct communication now.</p>"},{"location":"architecture/what-is-event-driven-architecture%3F/#conclusion","title":"Conclusion","text":"<p>Let\u2019s wrap it up here. Event Driven Design is good for complex systems and mature teams. But there is no silver bullet in software engineering, introducing new technology to the architecture has both pros and cons. What you need to do is understand the actual problem and solve it with less accidental complexity. If the team accepts the cost, then just go for it.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/why-do-we-need-architecture%3F/","title":"Why Do We Need Architecture?","text":"<p>As software engineers, dealing with software architecture is crucial in our day-to-day work. It involves making decisions about components like caches, message queues, search engines, or even transitioning from a monolithic to a microservices architecture. While we may know how to implement these components, it\u2019s important to understand why we need them.</p> <p>Let\u2019s consider the example of a cache. Some people believe that adding a cache can enhance read query performance, and since many popular companies utilize caches, it must be beneficial. While this may be true, it\u2019s essential to question whether your system truly needs a cache. Every architectural decision should be based on a specific reason. Well-known companies add caches to their systems because they have genuine needs, such as handling extremely high traffic where scaling traditional databases becomes challenging.</p> <p>In my opinion, architecture revolves around managing complexity. The core aspect of architecture is solving problems with minimal resources and complexity. Adding a cache undoubtedly introduces complexity to a system. If your system doesn\u2019t require a cache, there\u2019s no need to blindly follow others. However, understanding why others implement caches and the problems they solve can provide valuable insights into designing your system.</p> <p>Considering the cache example, the main reason for adding a cache is the improved read performance compared to traditional disk storage. However, the key message conveyed by caches is that scaling the cache component is easier than scaling traditional database storage. Additionally, caches can support higher concurrent traffic in high-traffic systems, thereby facilitating read performance.</p> <p></p> <p>In the context of a large traffic system, scaling a traditional database without using cache architecture can be extremely challenging in today\u2019s internet environment. Therefore, choosing a cache on top of a traditional database is beneficial because modern cache components support horizontal scaling, making it easier to scale compared to traditional databases like MySQL. While adding a cache introduces complexity, it is considerably less complex than scaling a traditional database.</p> <p>In summary, software architecture is vital for managing complexity and solving problems efficiently. Understanding the reasons behind architectural decisions, such as adding a cache, helps us make informed choices based on the specific needs and challenges of our system. By evaluating the benefits and trade-offs, we can design systems that perform optimally in high-traffic environments while effectively managing complexity.</p> <p> <p>Buy me a coffee </p>"},{"location":"architecture/why-do-we-need-webhook%3F/","title":"Why Do We Need Webhook?","text":"<p>In my last post of 2022, I explained the difference between API and Webhook. Today, I am going to create an actual scenario and show you how to use webhook and why we need it.</p> <p>Okay, let\u2019s get started. Before talking about Webhook, I want to bring the API approach up first. Let\u2019s take payment as an example.</p>"},{"location":"architecture/why-do-we-need-webhook%3F/#api","title":"API","text":"<p>The diagram above shows how the API approach works. Using this approach, the payment gateway needs to query the external payment service \u2014 ABC Payment to ask if the payment is done. As you can see in the diagram, the payment gateway keeps asking ABC Payment about the status of a specific payment. After 4 times, the payment gateway gets a success status from the ABC Payment.</p> <p>What is the drawback of this approach? Can you think about it?</p> <p>Ask yourself a question: How frequently the payment gateway will call the ABC Payment?</p> <p>This is unknown because the payment gateway does not know when the ABC Payment can finish a payment operation.</p> <p>If the payment gateway requests the ABC Payment too fast, it will cause a waste of resources on the payment gateway side. If the payment gateway requests too less, it will increase the payment latency.</p> <p>This approach is basically a pull model when 2 systems communicate with each other. Everything is controlled by the payment gateway side, but actually, the resource status is owned by ABC Payment. A consumer (Payment Gateway) never knows when the data will be updated on the provider (ABC Payment). If a consumer sends 10 calls to the provider, the first 9 calls will be wasted, and only the last one is valid. So, the pull model is not suitable for this use case.</p>"},{"location":"architecture/why-do-we-need-webhook%3F/#webhook","title":"Webhook","text":"<p>To solve this problem, we can use a push model which is Webhook.</p> <p></p> <p>Now, ABC Payment does not need to provide an API for their consumer. Instead, ABC Payment requires its consumers to provide an API for webhook callback. After the Payment Gateway invoked ABC Payment, the ABC Payment will trigger the webhook to callback the payment gateway when there is any payment status update event that happens on ABC Payment. After receiving the payment status from the ABC Payment, the Payment Gateway will update the payment status accordingly.</p> <p>That said, the event trigger is now controlled by the ABC Payment instead of the Payment Gateway. Now, the consumer does not need to manage the request frequency. Instead, it is managed by the provider.</p> <p>From the consumer\u2019s perspective, it just focuses on how to consume the data when the data is pushed to the consumer and does not need to manage extra complexity. </p> <p>On the other hand, from the provider\u2019s perspective, it needs to handle the high request rate when the consumer requests too fast in the pull model but now, it just needs to send a callback request to its consumers when the payment is done.</p>"},{"location":"architecture/why-do-we-need-webhook%3F/#conclusion","title":"Conclusion","text":"<p>Let\u2019s wrap it up. Using the right technology to solve the right problem can let the developers focus on the essential complexity which is the actual business problems. As you see, the architectural complexity decreased a lot on both sides by using webhook. The consumer does not need to design a scheduled job to query the provider while the provider does not need to worry about the consumer requesting too many.</p> <p> <p>Buy me a coffee </p>"},{"location":"code-review/author-journey/","title":"Code Author Journey","text":"<p>Code review is common in software development, especially working with a team. People want to ensure code quality and reasonable design during code review and align engineering practices with team standards.</p> <p>As a code author, you may receive a lot of comments from your teammates. Your teammate may suggest a better design for you, a better naming to describe a function, an improvement in code structure, and so on. Positive feedback can make you grow so don\u2019t feel that people are blaming you.</p> <p>Few things that I think it is important for a code author when creating Pull Request or your code is being reviewed by others:</p>"},{"location":"code-review/author-journey/#be-modest-and-open-minded","title":"Be modest and open-minded","text":"<p>As a software engineer or code author, being modest is really important when working with a team. I know you are good but you shouldn\u2019t think you are the best one and reject all the ideas from others. Good ideas come from discussion and communication. If your reviewers give you some feedback and it can help you improve your code readability, consistency, or correctness, why not just accept it? Running a code review is like knowledge sharing to help the team to build good engineering culture and share engineering thoughts with teammates. These opportunities can help you and your team grow together and build a solid codebase. So, be modest and open-minded and try to accept good feedback from others.</p>"},{"location":"code-review/author-journey/#write-a-descriptive-title-to-your-pull-request","title":"Write a descriptive title to your pull request","text":"<p>When creating a pull request, you are responsible to write a descriptive title to let your reviewers know the background of your changes like attaching your Jira ticket number, type of change, and a short description for the change. If the title is not enough to describe your change, you should write something in the description to provide more background information to your reviewers. It can facilitate communication between you and your reviewers and make the pull request get approved faster because your reviewers do not need to ask you what you are changing and why you need to make the change. Your time shouldn\u2019t be wasted in communicating the change background. So, make your pull request descriptive and help reviewers understand your changes are very important.</p>"},{"location":"code-review/author-journey/#write-small-code-changes","title":"Write small code changes","text":"<p>I know sometimes developers want to test the whole feature in the local environment before committing to Git. It is good that have tested on the local environment but remember if you push a large code change, people might feel it is hard to review. It takes a lot of time to understand a large code change saying 300+ lines. People usually do not want to review that large code change and they will simply leave LGTM to your pull request because it is hard to understand and takes a lot of time. Your reviewers are not only serving your pull request, they may have a lot of work pending to do. If you keep your code change small, people may find many issues for you and those comments can help you grow in your software engineering career. But, if you don\u2019t, probably people will not review your code deeply. You may lose some learning chances from your peer sometimes.</p> <p> <p>Buy me a coffee </p>"},{"location":"code-review/reviewer-journey/","title":"Reviewer Journey","text":"<p>As a code reviewer, you may give some comments or suggestions to the pull requests that you are reviewing. Giving feedback to others sounds not beneficial to you, but actually, it can help you improve your soft skills like communication skills and idea explanation skills and identify your blind spot on some concepts during explaining an idea. Also, good comments can help you build a good rapport with your peers and help the team build a solid codebase, and bring the team to the next level. Sometimes, you can learn some new ideas from your peers\u2019 code and discussion. So, don\u2019t be stingy about giving feedback.</p>"},{"location":"code-review/reviewer-journey/#be-polite-and-professional","title":"Be polite and professional","text":"<p>During code review, avoid using \u201cYou\u201d or \u201cYour\u201d to prevent giving people hard feelings. When leaving comments, you can try to use questions to ask the code author whether another approach will be better.  Also, you should provide more information and examples to help the code author to understand your idea. Using this approach, if the code author agrees with you, the code author will change the code immediately. If the code author does not agree with you, you can start a conversation with the code author to understand why he/she does not agree. Remember, take care of your peers\u2019 feelings, and don\u2019t be too harsh.</p> <p>Given that some items are just an improvement or nice-to-have, the reviewer can ask the code author to create a TODO item on the codebase and release the pull request first and clear the TODO item later instead of blocking the pull request to be merged.</p> <p>As a code reviewer, you should try to review the code as soon as possible and unblock your peer from the pull request if your schedule is not tight and time allows.</p>"},{"location":"code-review/reviewer-journey/#automate-style-checking","title":"Automate style checking","text":"<p>This is not really the responsibility of a reviewer but the whole team. To avoid subjective comments on styling issues, we should form some styling rules and get an agreement with the whole team about the coding standards and styling rules. After that, automate the style checking by using a linter plugin. We shouldn\u2019t waste time discussing style issues in a pull request. So, the reviewer can focus more on the actual concerns rather than formatting.</p> <p> <p>Buy me a coffee </p>"},{"location":"general/book-recomendation-1/","title":"3 non-technical books that influenced me","text":"<p>Nothing technical this week. I would like to recommend 3 non-technical books that I think it is worth reading as a software engineer.</p>"},{"location":"general/book-recomendation-1/#the-effective-engineer","title":"The Effective Engineer","text":"<p>https://www.effectiveengineer.com/book</p> <p>This book provides practical advice and insight for software engineers to become more effective including time management, prioritization, and career development. Also, it emphasizes the importance of business value and the impact of software engineering work. If you are looking for something that helps you scale yourself, this book might be useful to you.</p>"},{"location":"general/book-recomendation-1/#rework","title":"Rework","text":"<p>https://www.amazon.com/ReWork-Change-Way-Work-Forever/dp/0091929784</p> <p>This book talks about product development, hiring, company culture, and productivity. It promotes the concept of \u201cless is more\u201d and emphasizes simplicity, focus, and efficiency. This book is suitable for many people including people who want to build a team, people who want to develop a good product, and people who want to learn how to focus and improve efficiency.</p>"},{"location":"general/book-recomendation-1/#the-mythical-man-month","title":"The Mythical Man-Month","text":"<p>https://www.amazon.com/Mythical-Man-Month-Software-Engineering-Anniversary/dp/B005GM4EBS</p> <p>This book explores the challenges of managing a large complex project. It talks about project management, the work approach in a team, and communication. </p> <p>I like this sentence very much:</p> <p>\u201cMen and months are interchangeable commodities only when a task can be partitioned among many workers with no communication among them\u201d said by the author. </p> <p>This book is suitable for people who want to learn project management and communication in a team.</p> <p> <p>Buy me a coffee </p>"},{"location":"general/chatbot-driven-development/","title":"Chatbot Driven Development","text":"<p>Today, let\u2019s explore a little bit about how ChatGPT can help software engineers to boost their productivity and mentor inexperienced software engineers. There is a term called Chatbot Driven Development. That said, we can ask the chatbot to help us refactor our code or give the advice to facilitate our job. Let\u2019s get started with ChatGPT.</p> <p>My first question to ChatGPT was asking it to help me refactor my code for inserting a list of users into the database. In my original version, I intentionally use save method with a for-loop instead of using saveAll.</p> <p></p> <p>ChatGPT can help me correct my code and make it more efficient. Also, it explained a little bit about why it is efficient.</p> <p>After that, I asked another question: Why is saveAll efficient than save?</p> <p></p> <p>ChatGPT is able to explain the idea behind save and saveAll; why saveAll can help improve code performance and some potential risks when using save for batch insert.</p> <p>To extend the topic from the code level to the database level, I asked how large a batch can be handled in saveAll?</p> <p></p> <p>ChatGPT can answer it clearly and concisely and also bring out some databases that can handle large batches.</p> <p>I further asked whether MySQL can support handling large batches or not.</p> <p></p> <p>ChatGPT can provide some options to me and also pointed out we still need to consider memory and configuration of the MySQL server.</p> <p>I asked my last question to confirm the answer.</p> <p></p> <p>In my opinion, when asking questions, ChatGPT can summarize the answer and give you the answer directly. But if you don\u2019t know what to ask, ChatGPT might not be able to tell you the details. I feel like it is like a mentor for you to ask questions. If your command is clear, then ChatGPT definitely can help you boost your productivity and help you clear some software engineering concepts.</p> <p>Also, its ability for refactoring code is also good if you can give it some concrete commands. But if you don\u2019t have the skill to judge whether its answer is correct or not, it might have a problem when using it. So, I think building a good software engineering foundation is still important for software engineers. (To build a good software engineering foundation, you can subscribe to Awesome Software Engineer\ud83d\udca1) </p> <p>ChatGPT is a tool to help you do better instead of replacing you. In reality, companies still need humans to judge whether the code is correct but ChatGPT can give software engineers some insights into software engineering topics. If software engineers use this tool correctly, I am pretty sure that ChatGPT can boost engineers\u2019 productivity in solving some common problems in the software engineering area.</p> <p></p> <p> <p>Buy me a coffee </p>"},{"location":"general/serialization-vs-parallelization/","title":"Serialization vs Parallelization","text":""},{"location":"general/serialization-vs-parallelization/#serialization","title":"Serialization","text":"<p> Serialization means executing a series of tasks one after the other within a single thread. In other words, these tasks are executed sequentially, with one task starting only after the previous one has completed.</p>"},{"location":"general/serialization-vs-parallelization/#pros","title":"Pros","text":"<ul> <li> <p>Simple: Serailization is simple and straightforward to implement.</p> </li> <li> <p>Resource Efficiency: Unlike a multithreaded program, serialization uses a single thread to handle a series of tasks, so it requires fewer system resources.</p> </li> </ul>"},{"location":"general/serialization-vs-parallelization/#cons","title":"Cons","text":"<ul> <li> <p>Slower Execution: It executes the tasks one by one. So, it only starts a new task when the previous task has been completed.</p> </li> <li> <p>Inefficient Resource Utilization: Serialization may not efficiently utilize system resources in a system with a multicore CPU.</p> </li> </ul>"},{"location":"general/serialization-vs-parallelization/#parallelization","title":"Parallelization","text":"<p>Parallelization means executing a series of tasks in multiple threads, one task is assigned to a dedicated thread. In other words, these tasks are executed in parallel. This approach allows for better utilization of multiple CPU cores and can lead to improved performance and responsiveness in applications.</p>"},{"location":"general/serialization-vs-parallelization/#pros_1","title":"Pros","text":"<ul> <li> <p>Improved Performance: One of the primary benefits of parallelization is improved performance. By executing tasks concurrently on multiple CPU cores, you can significantly reduce the overall execution time of a program.</p> </li> <li> <p>Resource Utilization: Parallelization efficiently utilizes available hardware resources. It enables better use of multi-core processors and can lead to more efficient resource utilization in distributed computing environments.</p> </li> </ul>"},{"location":"general/serialization-vs-parallelization/#cons_1","title":"Cons","text":"<ul> <li> <p>Overhead: Managing threads comes with overhead, including context switching and memory usage. This overhead can offset performance gains for small tasks.</p> </li> <li> <p>Complexity: Parallelization introduces complexity into software design. Managing multiple threads, coordinating their activities, and handling synchronization can be challenging.</p> </li> <li> <p>Debugging: Debugging parallel code can be more challenging and time-consuming. Issues may be difficult to reproduce and diagnose.</p> </li> </ul>"},{"location":"general/serialization-vs-parallelization/#conclusion","title":"Conclusion","text":"<p>In summary, they both have their pros and cons. Choose the approach wisely depending on different use cases. The key is to understand the nature of your tasks and the trade-offs involved. In many cases, a combination of both techniques may be appropriate. Ultimately, the choice should align with your application\u2019s specific needs and goals.</p> <p> <p>Buy me a coffee </p>"},{"location":"mindset/design-thinking/","title":"Design Thinking: From Choas to Clarity","text":"<p>Chaos is everywhere in the tech industry, especially for creative and innovative work. When a problem comes, it is always ambiguous and large. How do you find the actual need in the problem and maximize the value?</p> <p>Given that you don\u2019t know whether your idea is right or not. In a large problem, should you focus on a small part or target the whole problem in one go?</p> <p>My choice is to focus on a small part at first and iterate it to validate the idea. You never know whether your idea will work or not without validating it. By focusing on a small part, we can keep the idea simple and keep iterating the idea to create the best solution for the problem.</p> <p>To maximize the value, you need to know whether you are solving the right problem. With experiments, you will make the ambiguous idea clear and create the best value.</p> <p>Personally, I will focus on the following 3 parts when I get a problem.</p>"},{"location":"mindset/design-thinking/#understand-the-problem","title":"Understand the problem","text":"<p>Before designing a solution, we need to understand the problem first. Sometimes, when the product manager gives you a requirement, it is not concrete enough. Engineers even didn\u2019t know whether it is actual user needs and what problem they are solving. To solve a problem, you need to know what is the problem. Working with a product manager, we need to learn how to ask questions. By asking the right questions, we will catch the nature of the problem.</p> <p>Also, we can narrow down the problem and filter out unnecessary requirements. Using this approach, we can fill the gap between tech and product and it helps the tech team to create the right solution for the right problem.</p> <p>If you don\u2019t ask questions, sometimes you can\u2019t understand the correct requirement from the product team and the product manager does not know the concerns of the tech team. Running this approach between tech and product, both parties might not know each other\u2019s concerns. For example, the tech team does not know the value of the product, and the product team does not understand the architecture limitation of tech. They are both executing commands without knowing the current foundation of the product no matter from tech or product.</p>"},{"location":"mindset/design-thinking/#define-scope","title":"Define scope","text":"<p>After we know the problem, we need to define a reasonable scope for the requirement because the problem might be large. The big scope will create a big problem because there are a lot of uncertainties and it is hard to control the quality, timeline, and efficiency. If we start working on the solution without breaking down the problem into small scopes, it might cause an uncontrollable timeline and incorrect user needs. Working on a solution, we might have unknowns/accidents that need to be solved during the development phase. If the scope is super big, it is hard to measure the workload and timeline. </p> <p>Also, working on a big scope will delay the time that the work actually goes live. We never know whether our product is good or not without letting real users use the product.</p> <p>Everything is just imagination before the work goes live.</p> <p>To validate our idea, we should break down the problem and validate the idea one by one in production. So, breaking down a problem into different subproblems is super important when building a product. By validating our work, the direction will become clear and we can know how many values we created from our work more importantly we know whether our work is actual user need or not.</p>"},{"location":"mindset/design-thinking/#iterate-the-solution","title":"Iterate the solution","text":"<p>Iterating the solution is super important since we broke down the problem into subproblems. We can roll out our solution to production and get feedback from real users to see if we can improve our solution.</p> <p>Also, if there are any issues in production, we can correct them more quickly. By iterating the solution, it can also give us to control the issue that might have in production because the scope is small. That said, we can better control the impact on production and validate our architecture on production. </p> <p>When we get a problem in production, we iterate the solution.</p> <p>By using this approach, the architecture/solution will get better and better and also we don\u2019t need to deal with a lot of unknowns at the same time.</p> <p>This can also lead to a better MVP solution and better utilize team resources.</p>"},{"location":"mindset/design-thinking/#summary","title":"Summary","text":"<p>To wrap up, never dive into a problem without asking questions to understand the nature of the problem and its scope. Instead of dealing with a large problem, try to break it down into different subproblems and iterate your solution to validate your idea.</p> <p> <p>Buy me a coffee </p>"},{"location":"mindset/direction/","title":"Navigating Complex Problem Spaces: Breakdown and Direction","text":"<p>In the realm of software engineering, a problem can often be immense. When embarking on problem-solving, comprehending the scope of the issue becomes paramount. This encompasses the problem space, a comprehensive domain encompassing all aspects of the problem. Imagine it as a landscape with various contours. As software engineers, it\u2019s crucial to skillfully deconstruct a problem into manageable fragments. This practice not only simplifies complexity but also allows teams to concentrate on specific and controllable components.</p> <p></p> <p>An immense problem space is a signal of potential intricacies and implies multiple avenues for resolution and various directions to explore. However, an abundance of choices without clear direction can confound the team. Visualize a map with numerous paths \u2013 without guidance, the team can lose their way, and decision-making becomes a daunting task. The result? A team adrift, grappling with an assortment of details and losing sight of the core issue at hand. So, what is the core problem that the team aims to solve? This is an important question that determines how far a team can go.</p> <p></p> <p>Contending with an extensive problem space necessitates a dual approach: breaking down and establishing direction. First, the task involves fragmenting the immense problem into smaller, more approachable sub-problems. Next comes the crucial step of defining a precise direction to guide the team\u2019s efforts. Ultimately, all the components are harmonized into a cohesive whole. This strategic methodology aligns with the age-old \u201cdivide and conquer\u201d philosophy.</p> <p>To make it short, the significance of these steps becomes apparent for problem-solving:</p> <ul> <li> <p>Problem Decomposition: Divide and Conquer emerge as a powerful technique for unraveling complex issues, transforming them into manageable parts.</p> </li> <li> <p>Prioritization: By prioritizing tasks, teams identify key objectives and allocate attention where it matters most.</p> </li> <li> <p>Decisive Action and Focus: A well-defined direction sharpens focus, streamlines tasks, and eliminates superfluous steps, guiding both team and product towards desired outcomes.</p> </li> <li> <p>Iterative Approach: Problem-solving rarely follows a straight line; iterative cycles enable adjustment and growth. Flexibility remains essential as teams refine strategies based on insights gained.</p> </li> </ul> <p>In summary, grappling with intricate problem spaces in software engineering necessitates a strategic approach. Understanding the scope of the issue, fragmenting it into manageable components, defining a clear direction, and embracing the \u201cdivide and conquer\u201d concept is integral to effective and efficient problem resolution.</p> <p> <p>Buy me a coffee </p>"},{"location":"mindset/everything-is-a-trade-off/","title":"Everything is a trade-off","text":"<p> Software engineering is all about choosing the right trade-off. No matter it is designing software architecture or writing code. Everything is a trade-off. There is no perfect solution. You always need to pick the right trade-off in your day-to-day routine.</p> <p>One of the examples is Performance vs Readability when writing code. Sometimes, if you choose Performance, you might lose some readability in your code, and vice-versa. Imagine if you are going to write an API with some high-performance algorithms. Now, things are getting complicated when using some complex algorithms in the code. Not everyone can understand your assumption and algorithm logic.</p> <p>What would you do? Should you go with Performance or Readability? That\u2019s the trade-off. Which one is right? It depends on which one your team thinks is important. If you go with Performance and your team can afford the price that losing the Readability, then that\u2019s it. Most of the time people want Readability more than Performance but if it affects a lot of user experience, people might go with Performance and sacrifice Readability. Software engineers should find the balance between Performance and Readability in terms of maintaining a reasonable performance and making people\u2019s life easier when reading the code.</p> <p>Another example is Distributed Transactions in microservices. In a microservices architecture, there might have many scenarios that need Distributed Transactions to ensure data consistency. But implementing Distributed Transactions is expensive. Would you implement it in every scenario? My answer is no. It needs a lot of tech effort and is hard to maintain the code. It will affect the team\u2019s development velocity and the whole solution is complicated. In most of the scenarios, we don\u2019t implement distributed transactions because the team cannot afford the price and it seems not valuable to implement distributed transactions in every possible scenario. Sometimes, we need availability more than consistency. So for this type of scenario, we just let it go. If it really happens, we just fix it manually. That\u2019s the trade-off that software engineers need to consider in their daily routines.</p> <p>What I have seen in my jobs is that people always think about designing a perfect solution to solve all the possible cases including edge cases. Eventually, they over-engineer the whole solution. What they are thinking is making the solution automated without human effort. I think this is good but it should depend on how much effort the team needs to put into the solution and whether is it valuable to put that much effort to solve an extreme edge case?</p> <p>Personally, I think we need to balance these 3 things:</p> <ul> <li> <p>Product Timeline: is the time that we spend on the solution acceptable for different product stakeholders?</p> </li> <li> <p>Performance: is the performance not acceptable?</p> </li> <li> <p>Developer Experience: is the solution hard to understand for other software engineers?</p> </li> </ul> <p>These factors will affect how you design the solution and we always need to balance these things. My idea is always to leave some room for the solution. Also, focus on essential complexity and iterate the solution when needed.</p> <p>Would you put 10x tech effort to solve an extreme edge case? or manually fix the edge case within a few minutes? Let me know your thoughts in the comment section.</p> <p> <p>Buy me a coffee </p>"},{"location":"mindset/how-do-you-delegate-task%3F/","title":"How Do You Delegate Task To Others?","text":"<p>Delegation stands as a critical skill that every software engineer should cultivate. I\u2019ve observed numerous instances where engineers delegate tasks but fail to maintain active involvement thereafter. This is not an effective way to delegate tasks and is not fair to others. In my opinion, delegating tasks is an opportunity to free one engineer and grow another engineer. Delegating tasks is not just about passing off work, but rather a strategic and collaborative approach to getting things done efficiently while fostering growth and learning.</p> <p>My approach to delegating tasks involves several key steps:</p>"},{"location":"mindset/how-do-you-delegate-task%3F/#gather-information-and-set-a-clear-expectation","title":"Gather information and set a clear expectation","text":"<p>When delegating a task to another engineer, there might be a lack of clarity on how to proceed. To address this, my usual approach involves gathering all necessary task-related information prior to delegation. This ensures that when the new engineer assumes the task, they have a comprehensive understanding of the required steps for its completion. Alternatively, It is better to outline specific instructions to guide them toward the relevant information sources. Subsequently, it becomes imperative to establish distinct expectations, encompassing the task\u2019s scope and anticipated outcomes. This approach fosters a mutual understanding of the intended results and minimizes any potential uncertainties. By adhering to this method, effective task delegation is achieved, circumventing the need for prolonged back-and-forth communication. Furthermore, this proactive clarity assures that the task progresses correctly and aligns precisely with the desired outcome.</p>"},{"location":"mindset/how-do-you-delegate-task%3F/#step-in-when-challenges-arise","title":"Step in when challenges arise","text":"<p>Step in when a task encounters obstacles or unforeseen incidents. Despite delegating the tasks, you retain ownership and bear the responsibility of assisting others in resolving blockers or unexpected situations. Your role involves ensuring a seamless engineering process during development and aiding your team\u2019s focus on their essential tasks.</p>"},{"location":"mindset/how-do-you-delegate-task%3F/#actively-progress-monitoring","title":"Actively progress monitoring","text":"<p>Delegation is not a complete transfer of ownership; instead, it expands the support network. Therefore, maintaining an active role in monitoring progress is crucial. Regular progress checks allow me to gauge the trajectory of tasks, verify alignment with objectives, and offer timely assistance as required.</p>"},{"location":"mindset/how-do-you-delegate-task%3F/#provide-constructive-feedback-during-codedesign-review","title":"Provide constructive feedback during code/design review","text":"<p>Delegation benefits both the person assigning tasks and the one receiving them. As a delegator, I gain the freedom to focus on important matters, while the delegatee gains valuable experience and added responsibilities. Giving positive feedback is crucial for their growth, and constructive feedback helps them improve further. Feedback also helps align expectations for outcomes. While it might require initial effort, the rewards increase over time. As the delegatee improves, the delegation process becomes smoother in the future.</p>"},{"location":"mindset/how-do-you-delegate-task%3F/#conclusion","title":"Conclusion","text":"<p>In summary, delegation is more than just giving tasks\u2014it\u2019s a collaborative strategy that promotes growth. By gathering information, staying engaged, tracking progress, and giving feedback, delegation becomes a rewarding experience for everyone. It helps with growth, efficient tasks, and teamwork in the ever-changing world of software engineering.</p> <p> <p>Buy me a coffee </p>"},{"location":"mindset/indirection/","title":"Any problem in computer science can be solved by another layer of indirection","text":"<p>David Wheeler: Any problem in computer science can be solved by another layer of indirection.</p> <p>Recently, I saw this quote on the internet, and it is really inspiring. I started thinking if this quote is really valid. Actually, Indirection means adding an extra layer. This technique is to decouple the dependency between the client and the target. To verify the idea, I defined several scenarios, and let\u2019s take the following examples to validate if the idea is really valid.</p>"},{"location":"mindset/indirection/#example-1-code-complexity","title":"Example 1: Code Complexity","text":"<p>Let\u2019s take code complexity as our first example. Imagine if we have a method with thousand lines of code. It is hard to understand, right?</p> <p>So, can we add another layer to solve this problem? The answer is Yes.</p> <p>We can extract the complex logic into different classes to shift the complexity and balance the complexity in different places.</p> <p>After that, we compose all the classes together to form the actual functionality.</p>"},{"location":"mindset/indirection/#example-2-traffic-pressure","title":"Example 2: Traffic Pressure","text":"<p>Imagine if your app is facing a traffic pressure problem and it cannot handle that many requests during peak hours. </p> <p>Can this problem be solved by adding another layer? The answer is Yes.</p> <p>Let\u2019s say we introduce Kafka into the app. The request now will not process synchronously, instead, it will go to Kafka and process asynchronously. That said, Kafka is the \u201canother layer\u201d to solve this problem. The requests will be stored in Kafka until a consumer consumes them.</p> <p>Another example is caching. Imagine a lot of \u201cRead\u201d requests hitting the database directly and the database becomes a system bottleneck now. To solve this problem, we can introduce another layer which is the cache layer to cache the data. Now, the requests will not hit the database directly if it is in the cache. Then, we can mitigate the database pressure.</p>"},{"location":"mindset/indirection/#example-3-incompatible-interface","title":"Example 3: Incompatible Interface","text":"<p>Imagine we are integrating 2 systems together, system 1 generates an XML file and system 2 receives a JSON file. Now, the interface is not compatible. You can\u2019t pass an XML to a JSON interface.</p> <p>How would you solve this problem? The answer is adding another layer.</p> <p>We can introduce a data conversion layer to convert an XML file to a JSON file and pass the converted JSON file to system 2.</p>"},{"location":"mindset/indirection/#example-4-add-extra-behavior","title":"Example 4: Add Extra Behavior","text":"<p>Imagine if you are working on a business project, and you need to create a bunch of APIs. On top of the APIs, you want to add extra behavior like logging for every API. But, you don\u2019t want to put logging into the API business logic because it will mess up the business logic.</p> <p>To solve this problem, we can add another layer. We can use Aspect-Oriented Programming (AOP) to solve the cross-cutting concern. By adding another layer, we don\u2019t need to mess up the business logic but still have API logging capability in another layer.</p> <p> <p>Buy me a coffee </p>"},{"location":"mindset/writing-tips/","title":"Tips for writing good documentation","text":"<p>Writing documentation is almost my day-to-day work as a software engineer. The reason why I write documentation is that good documentation can help my team understand the system better and it is like a knowledge-sharing process. One day if I am on vacation, my teammates can still gain the knowledge that they need from the documentation without asking me.</p> <p>I know not every engineer likes to create documentation. Some engineers might feel that creating documentation is just wasting time because the system is changing fast. That said, the documentation will be outdated fast. Yes, maintaining documentation is hard, and need to spend resources on that. But, when you work on a large system, good documentation can definitely facilitate team knowledge sharing and avoid some repetitive questions from others. It can help to free yourself from the loop of answering questions. </p> <p>The more you share, the more extra things you can create. That means when you share your knowledge with others to make them grow, you will save a lot of time. So, if you are working in a large complex system and the team is large, I do think investing in documentation is a good choice for your engineering team.</p> <p>From my experience in writing documentation, I summarized the following key points for writing good documentation:</p> <ol> <li> <p>Know your audience: Understand your audience\u2019s knowledge level and the specific needs of your audience. This will help you tailor your documentation to their expectation.</p> </li> <li> <p>Define your message: Clearly determine the core message or purpose of your documentation. What information do you want to deliver? Understanding your message will guide your writing and help you stay focused. If you don\u2019t define your message, sometimes, the content might be too rich and lose focus. With this writing style, your audience might not know what you want to deliver in the documentation. Ask yourself, what message you are trying to deliver when writing documentation.</p> </li> <li> <p>Tell a story: Frame your documentation in a narrative structure that engages readers. Start by presenting a problem and then providing the solution in a logical sequence For example, When documenting a system, introduce the problem, present the high-level architecture, explain critical scenario workflows, and delve into API and database designs. This approach guides readers to understand the system from a big picture to detailed aspects.</p> </li> <li> <p>Provide concise summaries: Instead of providing a lot of raw content to readers, help them summarize the content to give readers a quick overview and provide a link to the detailed content at the end, enabling them to find relevant information efficiently. This approach allows readers to grasp the main points quickly and explore further if desired.</p> </li> <li> <p>Visualize with diagrams: Instead of presenting a large block of content, leverage diagrams, flowcharts, or other visual representations to convey complex information. Visuals make it easier for readers to grasp concepts and understand relationships between different elements.</p> </li> <li> <p>Break down large documentation and create links: If you have extensive documentation, break it down into smaller, more manageable sections. Create separate documents for different topics and establish clear links or cross-references between them. This allows readers to access specific information quickly and navigate between related topics effortlessly.</p> </li> </ol> <p> <p>Buy me a coffee </p>"},{"location":"software-design/avoid-throwing-expcetion-as-control-flow/","title":"Avoid throwing exceptions as control flow","text":"<p>The exception is a mechanism to stop unexpected behavior in software development. When we get an unexpected behavior, we tend to throw an exception and let the exception handler process the error and return an elegant error message. But, I have seen developers abuse the exceptions in expected behaviors. For example, before creating a company, we need to validate if the company name is already taken. In this example, a company name getting registered by another user is possible and expected. </p> <p></p> <p>In the code snippet above, we throw an exception in the validation method and have a try-catch block on the caller method to handle the alternative flow.</p> <p>Is this really a good practice? Let\u2019s think about it.</p> <p>Personally, I think it is abusing exceptions. Because this behavior is allowed and expected in the requirement. Also, wrapping a try-catch block affects code readability and looks like there is an error occurs but actually it is not an error. It is quite misleading for readers. So, I think this style is not good and we should not throw an exception here.</p> <p>Now, we refactor the code structure to the below version which I think is better than the above and more descriptive:</p> <p></p> <p>In this refactored version, we remove the exception and try-catch block. Instead, we return a boolean in the validation method and rename it to isCompanyNameAlreadyTaken(\u2026). After that, we use the if statement to validate whether the name is taken or not. If it is already registered by someone, we generate a new name. Now, the logic and code structure both looks better than the previous ones because we removed the misleading information from the code. Also, the method name is more descriptive and the code structure is simpler. In this particular example, we can also inline the companyRepository.exist(name) method in the createCompany method to further simplify the code structure.</p> <p></p> <p>To sum up, if the behaviors are expected, try not to use exceptions. We can use the control flow statement to handle the logic. If you throw an exception and handle it by yourself, why don\u2019t just use the control flow statement to do the decision-making? There is no silver bullet for all use cases, in this particular use case, using the control flow statement is definitely better than throwing an exception. When implementing exceptions, if you have doubts, go backward and think again.</p> <p> <p>Buy me a coffee </p>"},{"location":"software-design/backward-compatibility-trade-off/","title":"Backward compatibility trade-off","text":"<p>What are the trade-offs of providing backward compatibility? In this post, I will talk about the backward compatibility trade-off.</p> <p>Okay, let\u2019s get started!</p>"},{"location":"software-design/backward-compatibility-trade-off/#increased-complexity","title":"Increased complexity","text":"<p>Handling backward compatibility requires the developers to design the APIs that are capable of multiple versions. Creating a backward-compatible API will increase the development time, code complexity, testing efforts, and maintenance costs. This will also make the work more challenging to maintain backward compatibility over time.</p>"},{"location":"software-design/backward-compatibility-trade-off/#increased-technical-debt","title":"Increased technical debt","text":"<p>Creating a backward-compatible API will increase technical debt because we need to support backward compatibility for a long time. If you are working with your internal frontend team, it may not affect you a lot. Because you can communicate with your internal developers to ask them to change their implementation. But if you are a service provider like Google or Amazon, you can not ask people to change their implementation immediately. That said, you might need to support multiple versions of API for a long time. Before that, you might need to think about when to deprecate the API and tell your customers that your old version of API is going to deprecate at what time.</p> <p> <p>Buy me a coffee </p>"},{"location":"software-design/backward-compatible-thinking/","title":"Backward Compatibility Thinking","text":"<p>Every software engineer should keep Backward Compatibility in mind during software development. If you want to provide a reliable service to others, you need to consider Backward Compatibility in your every change.</p> <p>So, what is Backward Compatibility? Actually, Backward Compatibility is not that fancy. It is just a term to describe that old data should compatible with the new code.</p> <p>This short statement implies a lot of things:</p> <p>Why do we need to ensure old data compatible with new code?</p> <p>Why do we have old data in the system?</p> <p>What will happen if we don\u2019t support backward compatibility?</p> <p>Is backward compatibility only affecting developers?</p> <p>Can you answer these questions?</p> <p>Why is backward compatibility so important to us especially building a product? The reason why we need to handle backward compatibility in our system is that there is a tricky moment exists during a deployment.</p> <p>When the new code is up, it is supposed to receive the latest data no matter from the data store or from the API consumer. But it is just an ideal situation. In reality, the API consumers are sending old data according to the old API contract or the data has not been migrated after deploying the code. In this situation, you must provide backward compatibility in your code. Otherwise, your system will fail when accepting old data and it might cause unreliable API and loss of trust from others.</p> <p>With backward compatibility provided, the system can:</p> <ul> <li>Provide reliable API to other consumers</li> <li>Avoid system failure internally, sometimes it is not API contract change but system logic/data change. We need to ensure our update is compatible with the previous version</li> <li>Build customer trust in your company</li> </ul> <p>Let\u2019s study a real scenario that happened in my project. Here is some context about the scenario.</p> <p>We migrate new data to an existing database table which introduces more columns and the columns are part of the logic in the new code. Assume now we have A, B, C, D, and E columns. D and E are newly added.</p> <p>We have a cache for the said data schema. Only A, B, and C are in the cache.</p> <p>We deploy our code to production with new logic that depends on D and E. After deploying, the incoming requests failed.</p> <p></p> <p>What? Why?</p> <p>I started tracing our code and turns out I found that the problem happen because of caching. Here is the workflow of the code:</p> <ol> <li> <p>The app tries to get the data either from the cache or the database by using the Cache-aside pattern</p> </li> <li> <p>The data exists in the cache and is read from the cache</p> </li> <li> <p>Remember, we don\u2019t have D and E in the cache \ud83d\ude28</p> </li> <li> <p>We use D and E in our code for some logical decision and turns out it causes NullPointerException \ud83d\ude30\ud83d\ude31</p> </li> </ol> <p>Imagine if your system needs to serve a large number of requests and now all the requests come and it fails. It can be terrible.\ud83d\ude23 </p> <p>Now, you know where is the problem. Do you know how can we avoid this issue? Leave your thoughts in the comment section.</p> <p>My takeaway from this accident:</p> <ul> <li>As a code reviewer, you might need to help think deeper when people introduce new fields into the existing data model.</li> <li>As a developer, you might need to think about any potential risks when introducing a new field. If you are not sure, maybe you can consult a more experienced developer.</li> <li>As a tech leader, you might need to help improve the skills of the whole team in order to understand the engineering fundamentals and summarize the failure and avoid the accident happen again.</li> </ul> <p>Failure is not terrible but it is terrible if the same failure happens again and again. When there is any failure, fix it and summarize your failure and improve yourself.</p> <p> <p>Buy me a coffee </p>"},{"location":"software-design/build-a-complex-object/","title":"Build a complex object","text":"<p>We have talked about the object creation approaches including Constructors and Static Factory Methods in the last post. But these approaches are more for simple object creation scenarios. </p> <p>What if today we need to create a complex object with some optional fields, how do we deal with this use case? Assuming we have 10+ or 20+ fields or we have different combinations. Now, we want to provide a flexible way for developers to create the object.</p> <p>Under this scenario, the constructor and static factory method are definitely not a good way to build such a complex object or offer good code readability. If you are going to use the constructor or static factory method, probably you need to create different variations with many parameters placed in the method signature or constructor. That sounds not a good way, because it makes people confused when they look at the method signatures or constructors.</p> <p>So, how do we solve this problem in an engineering way? Not constructor and not static factory method.</p> <p>You may say we can solve this problem with Setter. Yes, you can, but sometimes we want immutability for an object. That said, we don\u2019t want developers to mutate the object state after it creates. With this assumption, setter might not be a good choice for us because developers can mutate the object through the setters after it creates.</p> <p>The setter approach is banned. So, how to solve this problem gracefully? Fortunately, the builder pattern comes to the rescue in this use case.</p> <p></p> <p>Builder is a creational design pattern that lets you construct complex objects step by step. The pattern allows you to produce different types and representations of an object using the same construction\u00a0code. (Read more: https://refactoring.guru/design-patterns/builder)</p> <p>With the builder pattern, we can offer an elegant and flexible way to build a complex object. The developers can choose the fields that they are interested in the object and build the object step by step.</p> <p>Let us go through an example below to understand more about the builder pattern. (The example is using Java, sorry for non-Java guys)</p> <p>Builder Pattern Code Snippet: <pre><code>public class HttpUrl {\n    private final String schema;\n    private final String host;\n    private final String port;\n    private final String path;\n    private final Map&lt;String, String&gt; queryParams;\n\n    private HttpUrl(Builder builder) {\n        this.schema = builder.schema;\n        this.host = builder.host;\n        this.port = builder.port;\n        this.path = builder.path;\n        this.queryParams = builder.queryParams;\n    }\n\n    // omit implementation\n\n    public static class Builder {\n        private String schema;\n        private String host;\n        private String port;\n        private String path;\n        private Map&lt;String, String&gt; queryParams = new HashMap&lt;&gt;();\n\n        public Builder(String schema, String host) {\n            this.schema = schema;\n            this.host = host;\n        }\n\n        public Builder port(String port) {\n            this.port = port;\n            return this;\n        }\n\n        public Builder path(String path) {\n            this.path = path;\n            return this;\n        }\n\n        public Builder queryParam(String key, String value) {\n            this.queryParams.put(key, value);\n            return this;\n        }\n\n        public HttpUrl build() {\n            return new HttpUrl(this);\n        }\n    }\n}\n</code></pre></p> <p>We can put the mandatory fields in the constructor to restrict developers to fill in the fields. <pre><code>HttpUrl url = new HttpUrl.Builder(\"https\", \"awesomesoftwareengineer.com\").build();\n</code></pre></p> <p>Also, we can build some optional fields if needed.</p> <p><pre><code>HttpUrl url = new HttpUrl.Builder(\"https\", \"awesomesoftwareengineer.com\")\n                .port(\"8080\")\n                .path(\"/design-pattern\")\n                .queryParam(\"author\", \"ray\")\n                .build();\n</code></pre> Imagine if you don\u2019t use the builder pattern, how many constructors do you need to create for this use case? In this use case, we have only a few attributes but we have many different combinations. What if you are dealing with 10 or 20+ attributes? If we still use constructors, the code will be terrible and out of control.</p>"},{"location":"software-design/build-a-complex-object/#the-benefit","title":"The benefit","text":"<p>The builder pattern offers several benefits to us including:</p> <ul> <li> <p>Immutability: The object state cannot be updated after the object creation because we no longer need the setters to help us assign the values to the fields.</p> </li> <li> <p>Flexibility: When building a complex object with different combinations, the builder patterns can work perfectly because it offers a flexible way for developers to choose the fields that they are interested in and build the object step by step.</p> </li> <li> <p>Readability: Using the builder pattern, we don\u2019t need to create different constructors to cater to different scenarios. The methods are easy to read and human-friendly. It is almost like reading English.</p> </li> </ul>"},{"location":"software-design/build-a-complex-object/#the-drawback","title":"The drawback","text":"<p>Increases the overall code complexity after adopting the builder pattern but the cost is acceptable.</p>"},{"location":"software-design/build-a-complex-object/#summary","title":"Summary","text":"<p>Any design choice has a cost, it is not a free product. So, use the design pattern only if the price is acceptable and your team is willing to pay the price.</p> <p> <p>Buy me a coffee </p>"},{"location":"software-design/choose-the-right-way-to-create-objects/","title":"Choose the right way to create objects","text":"<p>In software development, we need to create different objects in our day-to-day work and it is so common, especially if you are using an object-oriented language. When talking about object creation, the first thing that comes to our mind is Constructor.</p> <p>You would say it is easy, we can use the constructor and put the required parameters into the constructor, and use the new keyword to instantiate the object. That\u2019s it.</p> <p>Yes, it is right in some sense. But what if you have different variations in the object? Some parameters are optional. How do you solve it?</p> <p>Now, you would say we can create different constructors to cater to different behaviors.</p> <p>Yes, it could be the solution. But, is this approach clean enough when people read your code?</p> <p>Let\u2019s take C# TimeSpan as an example: <pre><code>var duration1 = new TimeSpan(5);\nvar duration2 = new TimeSpan(0, 0, 5);\nvar duration3 = new TimeSpan(5, 0, 0);\nvar duration4 = new TimeSpan(5, 0, 0, 0, 0);\n</code></pre></p> <p>The above objects are created using the constructor. What is the problem here? Can you tell what is the meaning of the \u201c5\u201d in the 4 objects? Probably not, right? When using a constructor to instantiate different object variations, it basically hides the actual behavior and the meaning of the object itself. Everything becomes vague now, without comments we can\u2019t really understand what is happening here. The readability is not good because people need to think about the meaning of \u201c5\u201d or maybe need to dive into the source code of TimeSpan.</p> <p>If we don\u2019t use the constructor to instantiate the objects, what else can we choose? </p> <p></p> <p>Personally, I will choose the Static Factory Method for the less complicated objects like this case. With the static factory method, we can create different static methods in the object with a meaningful name. The static methods will help us construct the object and return the object to us. Using this, we hide the implementation details and shift the complexity to another place, the code readability gets improved like the following code snippet.</p> <pre><code>var duration1 = TimeSpan.FromTicks(5);\nvar duration2 = TimeSpan.FromSeconds(5);\nvar duration3 = TimeSpan.FromHours(5);\nvar duration4 = TimeSpan.FromDays(5);\n</code></pre> <p>Now, we can know the meaning of \u201c5\u201d in different objects and we don\u2019t need to put comments on different objects to explain what it is. Everything is quite obvious by using the static factory method.</p> <p>Sandi Metz: Design is thus about picking the right abstractions. If you choose well, your code will be expressive, understandable, and flexible, and everyone will love both it and you. However, if you get the abstractions wrong, your code will be convoluted, confusing, and costly, and your programming peers will hate you.</p> <p> <p>Buy me a coffee </p>"},{"location":"software-design/design-space-optimization/","title":"Design Space Optimization","text":"<p>In the realm of software engineering, two contrasting development approaches emerge: abstraction-driven design and simplicity-focused design.</p> <ul> <li>Abstraction-driven Design</li> </ul> <p>People who champion the abstraction-first method are all about thinking ahead. They\u2019re like the fortune-tellers of software - they try to predict future needs and design solutions that can be reused down the line. It\u2019s like building a universal toolkit for future problems. But here\u2019s the kicker: these predictions are made in a complex, ever-changing business world. Sometimes, reality takes a different route, and suddenly that fancy design doesn\u2019t fit the puzzle anymore and leads to a rigid design that is hard to adapt.</p> <ul> <li>Simplicity-focused Design</li> </ul> <p>On the flip side, we\u2019ve got the keep-it-simple believers. They\u2019re the pragmatists of the software engineering world. They say, \u201cHey, let\u2019s not complicate things until we have to.\u201d They build straightforward, no-nonsense solutions that are easy to wrap your head around. These guys are all about preserving design space for future extension. It\u2019s like having a roomy backpack that you can stuff with new tools whenever you need them. Of course, this approach comes with a catch \u2013 you need to roll up your sleeves and refactor stuff every now and then to stay up to date with the ever-changing business scene.</p> <p></p> <p>The diagram depicts the trajectory of the simplicity-focused approach. Beginning with a specific and tangible design, this methodology iteratively hones the solution over time. This approach resonates with scenarios where simplicity offers greater appeal than elaborate abstraction, except when abstraction genuinely simplifies matters.</p> <p>In a rapidly expanding product company, simplicity in design is key to allowing ample space for future iterations. Given the swift evolution of product requirements, a simple design approach is preferred. Delving too deeply into future possibilities from the outset can inadvertently constrain the design space, complicating understanding for others. This occurs when code becomes overly abstract and generalized.</p> <p>To elaborate, \u201cabstract and generic\u201d signifies an intention to address multiple potential problems, whereas \u201cconcrete and specific\u201d pertains to addressing a singular issue. Applying a versatile solution to a single problem elevates cognitive load \u2013 the mental effort required to comprehend the intricacies. Striking a balance between abstraction and specificity thus emerges as a crucial consideration in managing the design space effectively.</p> <p>The trade-off is evident: increasing abstraction might obscure comprehension due to numerous assumptions embedded in the design. Consequently, it reduces future expansion possibilities and amplifies the ripple effect of logic changes. In contrast, starting with a concrete solution promotes clarity, facilitating accessibility even for individuals lacking specialized backgrounds. This approach also fosters a more expansive design space, conducive to accommodating future extensions. Adaptation to evolving business needs can be achieved through strategic refactoring.</p> <p></p> <p>It\u2019s crucial to recognize that design space contracts over iterations as complexity accumulates. Establishing a refactoring culture becomes paramount to maintaining a reasonable design space and ensuring future extensions remain feasible. Typically, a concrete solution offers a broader design space, whereas abstraction\u2019s inherent assumptions restrict it.</p> <p>If you\u2019re finding it challenging to make changes when new requirements pop up, that could be a sign that your code needs some refactoring. It suggests that your design space might be too limited, making it tough to make necessary iterations smoothly.</p> <p>In conclusion, software engineering offers two distinct approaches: abstraction-driven design and simplicity-focused design. Striking a balance between predicting future needs and maintaining adaptability is key. The challenge lies in managing design space as complexity grows, necessitating occasional refactoring. Agility, openness to change, and a pragmatic mindset are essential for optimizing design and creating resilient software solutions that can evolve alongside evolving business requirements.</p> <p> <p>Buy me a coffee </p>"},{"location":"software-design/gracefully-handle-dynamic-behavior/","title":"Gracefully handle dynamic behaviors in the same category","text":"<p>As software engineers, you might need to handle different behaviors in the same category in your day-to-day work.</p> <p>Imagine if you are working on a notification integration service and there are different communication channels including SMS, Email, and Slack that needs to be integrated into the system.</p> <p>How do you organize your code to route a notification to different channels?</p> <p>The first thing that comes to our mind is the control flow statement. We can use the simple if-else statement to make the decision in choosing different channels.</p> <p></p> <p>It is good to keep things simple at the beginning stage and seems this is the best choice at this stage because we have 3 communication channels only. Writing simple if-else is the most straightforward and readable approach.</p> <p>Few months pass. Now, the requirement changed. The product manager plans to add more communication channels to support the operation, for example, Line, Telegram, Signal, and some other else channels. The communication channels getting more and more. Every time we integrate a new communication channel, the caller method needs to update the control flow statement and the decision-making flow is getting longer and longer.</p> <p></p> <p>When looking at the code above, it seems the caller method is too complicated now. Also, does the caller method care about these implementation details? Probably not. The control flow is stable, only the implementation is different. Imagine you are reviewing the caller method, do you really care about the implementation of different channels? Probably what you care about is how to determine the notification channel and how to compose a notification message. For sending a message to an external, we don\u2019t really care the sending behavior as long as the app can send the notification out.</p> <p>Having these assumptions, we shouldn\u2019t put the decision-making flow and implementation details in this caller method because it increases the complexity of the caller method and people do not really care about the details.</p> <p>To solve this problem, the Strategy Pattern comes to the rescue. We can abstract a notification handler with common attributes and encapsulate the implementation details into different notification handlers like EmailNotificationHandler, SlackNotificationHandler, SmsNotificationHandler, etc.</p> <p>  Next step, we can have a registry class for registering channels and handlers and it will be a map data structure. A channel will be mapped to a specific handler.</p> <p></p> <p>Now, you can simply look up a notification channel through this map. The logic in the caller method will become something like this:</p> <p></p> <p>Basically, we are adding one more layer to shift the complexity from the caller method to the downstream. In the future, if more new channels add to the app, we don\u2019t need to touch the caller method. What we need to touch on is creating a new notification handler for that channel and registering the implementation into the registry class. The strategy pattern brings some abstraction cost to the codebase but the complexity of the strategy pattern is not very high, most of the teams are able to pay the cost.</p> <p>This pattern is very useful to organize different behavior in the same category. Another example is the payment channel. Imagine if you need to integrate different payment gateways into your app, you will have the same situation as the above notification channel scenario.</p> <p>When the complexity gets large, we tend to shift the complexity to different places to mitigate the overall complexity in a particular place. It is common in software engineering, we are always making trade-offs in our day-to-day work.</p> <p>For more information about the strategy pattern, check out: https://refactoring.guru/design-patterns/strategy</p>"},{"location":"software-design/gracefully-handle-dynamic-behavior/#any-design-choice-has-a-cost-it-is-not-a-free-product-so-use-the-design-pattern-only-if-the-price-is-acceptable-and-your-team-is-willing-to-pay-the-price","title":"Any design choice has a cost, it is not a free product. So, use the design pattern only if the price is acceptable and your team is willing to pay the price.","text":"<p> <p>Buy me a coffee </p>"},{"location":"software-design/prefer-duplication-over-wrong-abstraction/","title":"Prefer Duplication over Wrong Abstraction","text":"<p>In software development, engineers tend to create abstractions to make the code reusable and avoid duplication. It is good in most of the use cases but remembers, the product requirement is changing fast, if you create an abstraction at the very beginning, you are very likely to fall into a trap \u2014 The Wrong Abstraction. </p> <p>If you keep working on the wrong abstraction, in the end, you will lose control of your codebase. No one can understand the codebase and you need a lot of documentation, comments, and diagrams to help you understand the existing behaviors. In order to reduce a few lines of code but create a wrong abstraction, it will make your code incomprehensible and unmaintainable.</p> <p></p> <p>Isn\u2019t it Code as Documentation?</p> <p>Imagine one of the use cases:</p> <p>Engineer A sees a duplication in the codebase, he abstracts it</p> <p><pre><code>public void doSomething(String param1, String param2) {\n    // A...\n    // B...\n    // C...\n    // D...\n    // E...\n}\n</code></pre> Time passes and new requirements come</p> <p>Engineer B sees the new requirements almost fit the current abstraction and he adds one more parameter to determine the flow</p> <pre><code>public void doSomething(String param1, String param2, boolean flag) {\n    // A...\n    if (!flag) {\n        // B...\n    }\n    // C...\n    // D...\n    // E...\n}\n</code></pre> <p>Now, another requirement comes</p> <p>Engineer C adds one more parameter to the code and introduces another condition to cater to the new requirement <pre><code>public void doSomething(String param1, String param2, boolean flag, boolean flag2) {\n    // A...\n    if (!flag) {\n        // B...\n    }\n    if (!flag2) {\n        // C...\n    }\n    // D...\n    // E...\n}\n</code></pre></p> <p>Do you know what\u2019s the problem in the code above? </p> <p>It is creating the Wrong Abstraction. People just want to reuse the code all the way but without considering the actual situation. In the end, they lose all code readability because of the wrong abstraction. It is so common in software development. Have you thought about this problem? How can you make it better?</p> <p>If you face the situation, remember to go backward and create duplication. It helps readability and maintainability. Rethink how to create a better design when the requirement is stable enough.</p> <p>Sandi Metz summarized this with a beautiful description: prefer duplication over the wrong abstraction</p> <p>This principle is also called AHA (Avoid Hasty Abstractions) which means keeping everything simple at the early stage even if it is duplicated.</p>"},{"location":"software-design/prefer-duplication-over-wrong-abstraction/#conclusion","title":"Conclusion","text":"<p>To embrace code readability and maintainability, we should avoid hasty abstraction and make everything as simple as possible at the early stage even if using a naive approach.</p> <p> <p>Buy me a coffee </p>"},{"location":"software-design/simple-is-the-best/","title":"Design Principles: Simple is the best","text":"<p>In this post, we are going to discuss You Aren\u2019t Gonna Need It. I find this principle useful for software development because the requirement is changing fast and this principle helps a lot.</p> <p>Okay, let\u2019s jump into the topic.</p> <p></p> <p>You Aren\u2019t Gonna Need It (YAGNI) is a design principle of Extreme Programming. This principle expresses the idea:</p> <p>Always implement things when you actually\u00a0need them, never when you just foresee\u00a0that you may need them.</p> <p>That said, you shouldn\u2019t make any assumptions to predict the future product requirement. Especially working in a fast-growing company, the software keeps iterating and the product requirement keeps changing. If you make a prediction on product requirements and develop your predicted requirement into the feature, probably the feature is over-engineered.</p> <p>Over-engineering will make your solution much more complicated and take a lot of time to implement the solution when compared to a simple and naive solution. If there is a simple and straightforward approach that can solve the problem, why not just adopt it? Over designing a feature can slow down the team development velocity and productivity. Also, it makes other team members hard to understand the solution because you made some assumptions about your solution but others do not have those assumptions.</p> <p>If now the product requirements changed and it breaks your assumptions, your efforts on this over-engineered feature will be wasted and it takes a lot of time to correct the behavior.</p> <p>With YAGNI, we never predict product requirements and keep everything as simple as possible. We only address the requirements that we know about and do only absolutely necessary things to avoid waste. Adopting this principle can increase the maintainability and easier to support and revise the code in the future. If things change, we can update the behavior easily, or else throw it away in the worst case. It does not hurt too much on our development and we will not waste a lot of unnecessary efforts on the software development process.</p> <p>Another similar idea: KISS Principle Read more: https://en.wikipedia.org/wiki/KISS_principle</p> <p> <p>Buy me a coffee </p>"}]}